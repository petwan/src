---
title: âš¡MiniGPT â€”â€” åŸºäºä»£ç ç†è§£ transformer
date: 2026-01-05
tags: [LLMs, Pytorch]
description: ä½¿ç”¨ Pytorch ä»é›¶æ„å»ºè¯è¡¨ã€æ„å»º decoder-only çš„ ç±»GPT-2 æ¨¡å‹ï¼Œä» 0 åˆ° 1 å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„å¯¹è¯æ¨¡å‹ã€‚æ¨¡å‹æ•´ä½“ Transformer Only DecoderÂ ä½œä¸ºæ ¸å¿ƒæ¶æ„ï¼Œç”±å¤šä¸ªç›¸åŒçš„å±‚å †å è€Œæˆï¼Œæ¯ä¸ªå±‚åŒ…æ‹¬è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€ä½ç½®ç¼–ç å’Œå‰é¦ˆç¥ç»ç½‘ç»œã€‚
draft: false
---

# âš¡MiniGPT â€”â€” åŸºäºä»£ç ç†è§£ transformer

> è¿™æ˜¯ä¸€ç¯‡è®°å½•è‡ªå·±ç†è§£ transformer æ¨¡å‹çš„ç¬”è®°ï¼Œä¸»è¦æ˜¯ä½¿ç”¨Pytorchå®ç°ä¸€ä¸ªåŸºç¡€çš„ GPT-2 æ¨¡å‹ã€‚

## 1. æ•°æ®é›†
ä½¿ç”¨å¯¹è¯-ç™¾ç§‘ï¼ˆä¸­æ–‡ï¼‰æ•°æ®é›†ï¼Œæ¶µç›–äº†ç¾é£Ÿã€åŸå¸‚ã€ä¼ä¸šå®¶ã€æ±½è½¦ã€æ˜æ˜Ÿå…«å¦ã€ç”Ÿæ´»å¸¸è¯†ã€æ—¥å¸¸å¯¹è¯Â ç­‰ä¿¡æ¯ã€‚æ•°æ®é›†ä¸‹è½½åœ°å€ï¼š[here](https://modelscope.cn/datasets/qiaojiedongfeng/qiaojiedongfeng/summary)

> è¿™ä¸ªæ•°æ®é›†åœ¨ minigpt çš„ä»£ç ä¸­å·²ç»åŒ…å«ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚

æ•°æ®æ ¼å¼å¦‚ä¸‹ï¼š
```console
{"question": "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ", "answer": "ä½ å¥½ï¼æˆ‘æœ€è¿‘è¿˜ä¸é”™ï¼Œè°¢è°¢ã€‚"}
{"question": "ä»Šå¤©å¤©æ°”å¦‚ä½•ï¼Ÿ", "answer": "ä»Šå¤©çš„å¤©æ°”å¾ˆæ™´æœ—ã€‚"}
{"question": "ä½ å–œæ¬¢æ—…è¡Œå—ï¼Ÿ", "answer": "æ˜¯çš„ï¼Œæˆ‘éå¸¸å–œæ¬¢æ—…è¡Œã€‚"}
{"question": "ä½ æœ€å–œæ¬¢çš„é£Ÿç‰©æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æˆ‘æœ€å–œæ¬¢çš„é£Ÿç‰©æ˜¯å¯¿å¸ã€‚"}
{"question": "ä½ æœ‰ä»€ä¹ˆå…´è¶£çˆ±å¥½ï¼Ÿ", "answer": "æˆ‘å–œæ¬¢é˜…è¯»å’Œè¿åŠ¨ã€‚"}
{"question": "ä½ æœ€å–œæ¬¢çš„ç”µå½±æ˜¯ä»€ä¹ˆï¼Ÿ", "answer": "æˆ‘æœ€å–œæ¬¢çš„ç”µå½±æ˜¯ã€Šè‚–ç”³å…‹çš„æ•‘èµã€‹ã€‚"}
```

## 2. æ„å»ºè¯è¡¨
å› ä¸ºæ•°æ®é›†æ˜¯ä¸­æ–‡ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ä¸€ä¸ªå­—ä½œä¸ºä¸€ä¸ªè¯ï¼Œå¹¶åœ¨è¿™ä¸ªåŸºç¡€ä¸ŠæŠŠæ ‡ç‚¹ç¬¦å·ä»¥åŠè¡¨æƒ…ç¬¦å·éƒ½çº³å…¥è¯è¡¨ä¸­ã€‚åŒæ—¶ï¼Œè¯è¡¨ä¸­è¿˜éœ€è¦ä¸‰ä¸ªç‰¹æ®Šçš„è¯ï¼š `<pad>`ç”¨äºè¡¨ç¤ºå ä½ã€`<unk>` ç”¨äºè¡¨ç¤ºæœªçŸ¥ã€ `<sep>`è¡¨ç¤ºåˆ†éš”ç¬¦ï¼Œç”¨äºåˆ†éš” question å’Œ answerã€‚

- **ç¤ºä¾‹**ï¼š
  - æ–‡æœ¬ï¼š`"ä½ å¥½å—ï¼Ÿ"`
  - åˆ†è¯ç»“æœï¼š`["ä½ ", "å¥½", "å—", "ï¼Ÿ"]`
  - æ¯ä¸ªå­—å¯¹åº”ä¸€ä¸ª ID

```python
import json
import argparse
from collections import Counter


def build_vocab(data_path: str, output_path: str):
    counter = Counter()

    with open(data_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            try:
                item = json.loads(line)
                # æ”¶é›† question å’Œ answer ä¸­çš„æ‰€æœ‰å­—ç¬¦
                counter.update(item["question"])
                counter.update(item["answer"])
            except Exception as e:
                print(f"Warning: skip invalid line: {line[:50]}... | Error: {e}")

    # è·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦
    chars = sorted(counter.keys())

    # æ·»åŠ ç‰¹æ®Š token
    special_tokens = ["<pad>", "<unk>", "<sep>"]

    # æ„å»º word2idï¼šå…ˆæ”¾ç‰¹æ®Š tokenï¼Œå†æ”¾å­—ç¬¦ï¼ˆé¡ºåºå›ºå®šä¾¿äºå¤ç°ï¼‰
    word2id = {token: i for i, token in enumerate(special_tokens)}
    for char in chars:
        if char not in special_tokens:  # é˜²å¾¡ï¼šè·³è¿‡ç‰¹æ®Š token
            word2id[char] = len(word2id)  # è‡ªåŠ¨é€’å¢

    # æ„å»º id2word
    id2word = {i: token for token, i in word2id.items()}

    vocab = {"word2id": word2id, "id2word": id2word}

    # ä¿å­˜ vocab
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=4)

    print(f"âœ… Vocabulary built and saved to {output_path}")
    print(f"   Total tokens: {len(word2id)}")
    print(f"   Special tokens: {special_tokens}")
    print(f"   Sample chars: {chars[:10]}...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build vocabulary from QA dataset.")
    parser.add_argument(
        "--data", type=str, required=True, help="Path to training data (JSONL format)"
    )
    parser.add_argument(
        "--output", type=str, default="vocab.json", help="Output vocab file path"
    )
    args = parser.parse_args()

    build_vocab(args.data, args.output)
```
æ‰§è¡Œå‘½ä»¤åå°†è¯è¡¨ä¿å­˜åœ¨dataç›®å½•ä¸‹ï¼Œç”Ÿæˆçš„è¯ä¸€å…±4825ä¸ªï¼Œè¯¥è¯è¡¨å°†è¢« `Tokenizer` ç±»åŠ è½½ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„ token ID åºåˆ—ã€‚

```bash
python ./minigpt/build_vocab.py --data ./data/data.jsonl --output ./data/vocab.json
```

## 3. åˆ›å»º Tokenizer ç±»
Tokenizer ç±»ç”¨äºå°†è¾“å…¥çš„æ–‡æœ¬æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„ç´¢å¼•åºåˆ—ï¼ŒåŒæ—¶å°†æ¨¡å‹çš„è¾“å‡ºè½¬æ¢ä¸ºå¯è¯»æ–‡æœ¬ã€‚

```python
import json


class Tokenizer:
    def __init__(self, vocab_path: str):
        with open(vocab_path, "r", encoding="utf-8") as f:
            vocab = json.load(f)

        self.word2id = vocab["word2id"]
        self.id2word = {int(k): v for k, v in vocab["id2word"].items()}

        # å›ºå®šç‰¹æ®Š token ID
        self.pad_token_id = self.word2id["<pad>"]
        self.unk_token_id = self.word2id["<unk>"]
        self.sep_token_id = self.word2id["<sep>"]

    def encode(
        self,
        question: str,
        answer: str,
        max_length: int = 128,
        pad_to_max_length: bool = True,
    ):
        """å°†é—®ç­”å¯¹ç¼–ç ä¸º token ID åºåˆ—ã€‚"""
        tokens = []

        # encode question
        for char in question:
            tokens.append(self.word2id.get(char, self.unk_token_id))
        tokens.append(self.sep_token_id)  # æ·»åŠ åˆ†éš”ç¬¦

        # encode answer
        if answer is not None:
            for char in answer:
                tokens.append(self.word2id.get(char, self.unk_token_id))

            tokens.append(self.sep_token_id)

        # æ„å»º attention maskï¼ˆ1=çœŸå® tokenï¼Œ0=paddingï¼‰
        attn_mask = [1] * len(tokens)

        # æˆªæ–­æˆ–å¡«å……
        if pad_to_max_length:
            if len(tokens) > max_length:
                # æˆªæ–­ï¼ˆä¿ç•™å¼€å¤´ï¼‰
                tokens = tokens[:max_length]
                attn_mask = attn_mask[:max_length]
            else:
                # å¡«å……
                pad_len = max_length - len(tokens)
                tokens.extend([self.pad_token_id] * pad_len)
                attn_mask.extend([0] * pad_len)

        return tokens, attn_mask

    def decode(self, ids):
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºåŸå§‹æ–‡æœ¬ï¼ˆè·³è¿‡ <pad>ï¼‰ã€‚"""
        return "".join(
            self.id2word[i] for i in ids if i != self.pad_token_id  # è·³è¿‡å¡«å……ç¬¦
        )

    def get_vocab_size(self):
        return len(self.id2word)
```

ä½¿ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥æµ‹è¯•è¿™ä¸ªç±»ï¼š
```python
if __name__ == "__main__":
    question = "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"
    answer = "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"

    tokenizer = Tokenizer("./data/vocab.json")

    input_ids, attn_mask = tokenizer.encode(question, answer, max_length=32)
    print(input_ids)
    print(tokenizer.decode(input_ids))
    """------ result ------
    [368, 1086, 4810, 2005, 4169, 1521, 240, 2103, 4816, 2, 1646, 1480, 1086, 4810, 3989, 3989, 4806, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ<sep>æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼<sep>
    """
```

## 4. åˆ’åˆ†æ•°æ®
å°†æ•°æ®æ‰“ä¹±åè¿›è¡Œåˆ’åˆ†ï¼Œè¿™é‡Œæˆ‘åˆ’åˆ†æˆ80%è®­ç»ƒé›†å’Œ20%éªŒè¯é›†ã€‚
```python
# split_data.py
import json
import argparse
import random
from pathlib import Path


def split_jsonl_data(input_path: str, train_ratio: float = 0.9, seed: int = 42):
    """
    å°† JSONL æ ¼å¼çš„ QA æ•°æ®é›†åˆ’åˆ†ä¸º train.jsonl å’Œ val.jsonlã€‚

    Args:
        input_path (str): åŸå§‹æ•°æ®è·¯å¾„ï¼ˆJSONLï¼‰
        train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0.0 ï½ 1.0ï¼‰
        seed (int): éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    """
    input_path = Path(input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # è¯»å–æ‰€æœ‰æœ‰æ•ˆè¡Œ
    data = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                if "question" in item and "answer" in item:
                    data.append(line)  # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²ï¼Œé¿å…æ ¼å¼å˜åŒ–
                else:
                    print(
                        f"âš ï¸  Warning: Line {line_num} missing 'question' or 'answer', skipped."
                    )
            except json.JSONDecodeError:
                print(f"âš ï¸  Warning: Line {line_num} is invalid JSON, skipped.")

    if not data:
        raise ValueError("No valid data found!")

    # æ‰“ä¹±å¹¶åˆ’åˆ†
    random.seed(seed)
    random.shuffle(data)
    n_train = int(len(data) * train_ratio)

    train_data = data[:n_train]
    val_data = data[n_train:]

    # è¾“å‡ºè·¯å¾„
    output_dir = input_path.parent
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"

    # å†™å…¥æ–‡ä»¶
    with open(train_path, "w", encoding="utf-8") as f:
        f.write("\n".join(train_data) + "\n")
    with open(val_path, "w", encoding="utf-8") as f:
        f.write("\n".join(val_data) + "\n")

    print("âœ… Split completed!")
    print(f"   Total samples: {len(data)}")
    print(f"   Train: {len(train_data)} â†’ {train_path}")
    print(f"   Val:   {len(val_data)} â†’ {val_path}")
    print(f"   Train ratio: {train_ratio:.1%}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Split QA dataset into train/val sets."
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Path to original JSONL dataset (e.g., data/all.jsonl)",
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.9,
        help="Proportion of data to use for training (default: 0.9)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility (default: 42)",
    )
    args = parser.parse_args()

    split_jsonl_data(args.input, args.train_ratio, args.seed)
```

æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æ‹†åˆ†æ•°æ®é›†ï¼Œå¦‚æœæ˜¯åœ¨æ²¡æœ‰GPUèµ„æºæˆ–GPUæ˜¾å­˜ä¸è¶³çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è°ƒæ•´ä»£ç ï¼Œå–ä¸€å°éƒ¨åˆ†æ•°æ®è¿›è¡Œæµ‹è¯•ã€‚
```bash
python ./minigpt/split_data.py --input ./data/data.jsonl --train_ratio 0.8
```

## 5. æŸ¥çœ‹è®­ç»ƒæ•°æ®çš„tokenç»Ÿè®¡

æ‰§è¡Œå¦‚ä¸‹ä»£ç ï¼ŒæŸ¥çœ‹è®­ç»ƒæ•°æ®çš„tokenç»Ÿè®¡ä¿¡æ¯ã€‚
```bash
python ./minigpt/dataset_stats.py --train_data ./data/train.jsonl --vocab_path ./data/vocab.json
```

å¯ä»¥çœ‹åˆ°95%çš„tokené•¿åº¦éƒ½åœ¨140ä»¥å†…ï¼Œå› æ­¤åé¢æˆ‘ä»¬åœ¨åé¢è®­ç»ƒçš„æ—¶å€™ï¼Œå¯ä»¥å°†æœ€å¤§çš„è¾“å…¥é•¿åº¦è®¾ç½®ä¸º140ã€‚

<Image 
src='assets/minigpt_dataset_stats.png'
card=true
/>

## 6. åˆ›å»º Dataset
è¿™é‡Œæ²¡æœ‰è€ƒè™‘å¤ªå¤šä¼˜åŒ–çš„é—®é¢˜ï¼Œä»…åšå­¦ä¹ ï¼Œå› æ­¤åˆ›å»ºçš„QADatasetç±»æ¯”è¾ƒç®€å•ã€‚

```python
import torch
from torch.utils.data import Dataset
import json
from minigpt.tokenizer import Tokenizer


class QADataset(Dataset):
    """
    è‡ªå›å½’è®­ç»ƒç”¨çš„é—®ç­”æ•°æ®é›†ï¼ˆQuestion-Answer Datasetï¼‰ã€‚

    å°† (question, answer) æ‹¼æ¥ä¸ºå•ä¸ªåºåˆ—ï¼Œå¹¶æ„é€ ï¼š
      - input_ids:   [SOS, q1, q2, ..., <sep>, a1, a2, ..., <sep>]
      - targets:     [q1, q2, ..., <sep>, a1, a2, ..., <sep>, EOS]

    å®é™…é€šè¿‡ shift å®ç°ï¼šinput = tokens[:-1], target = tokens[1:]
    """

    def __init__(self, data_path: str, tokenizer: Tokenizer, max_length: int):
        self.tokeniner = tokenizer
        self.max_length = max_length
        self.data = []

        print(f"Loading data from {data_path}")

        with open(data_path, "r", encoding="utf-8") as f:
            # é€è¡Œè¯»å–æ•°æ®, è¡Œæ•°ä»1å¼€å§‹
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    item = json.loads(line)
                    if "question" not in item or "answer" not in item:
                        print(
                            f"âš ï¸  Line {line_num}: Missing 'question' or 'answer', skipped."
                        )
                        continue
                    self.data.append((item["question"], item["answer"]))

                except Exception as e:
                    print(f"âš ï¸  Line {line_num}: Invalid JSON, skipped. Error: {e}")

        print(f"âœ… Loaded {len(self.data)} valid QA pairs.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question, answer = self.data[idx]
        # encode question and answer
        full_tokens, atnn_mask = self.tokeniner.encode(
            question, answer, max_length=self.max_length
        )

        # è‡ªå›å½’è®­ç»ƒï¼š input å‘å³ç§»ä¸€ä½ï¼Œtarget å‘å·¦ç§»ä¸€ä½
        input_ids = full_tokens[:-1]  # å»æ‰æœ€åä¸€ä¸ª token
        attention_mask = atnn_mask[:-1]  # å¯¹åº” input_ids çš„ attention mask
        targets = full_tokens[1:]  # å»æ‰ç¬¬ä¸€ä¸ª token

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "targets": torch.tensor(targets, dtype=torch.long),
        }

if __name__ == "__main__":
    tokenizer = Tokenizer("./data/vocab.json")
    dataset = QADataset("./data/train.jsonl", tokenizer, max_length=128)

    print(f"æ•°æ®é›†å¤§å°ï¼š{len(dataset)}")

    item = dataset[0]
    print(tokenizer.decode(item["input_ids"].tolist()))
    print(tokenizer.decode(item["targets"].tolist()))
```

æˆ‘ä»¬çœ‹ä¸€ä¸‹ idx ä¸º 0 çš„æ ·æœ¬ä¸­æ•°æ®ç»è¿‡ decode åçš„ç»“æœï¼š

```console
æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©åˆ†ç±»ä¸ºä½•ç±»ï¼Ÿ<sep>æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©å±äºæœå®ç±»ã€‚<sep>
æ ‘ç”Ÿé•¿çš„äº§ç‰©åˆ†ç±»ä¸ºä½•ç±»ï¼Ÿ<sep>æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©å±äºæœå®ç±»ã€‚<sep>
```

## 7. æ¨¡å‹æ­å»º
### 7.1 æ³¨æ„åŠ›æœºåˆ¶èƒŒæ™¯

é¦–å…ˆç”¨ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­è¯´æ˜æ³¨æ„åŠ›å±‚

- **Query**: é€šè¿‡å¤–å–Appç‚¹å¤–å–ï¼Œä¸ªäººçš„å…³æ³¨ç‚¹æ˜¯é¥­åº—å£å‘³æ˜¯å¦éº»è¾£å’Œä»·æ ¼ï¼Œä¾‹å¦‚ Query = [0.9, 0.8] è¡¨ç¤ºæˆ‘å¸Œæœ›é¥­åº—çš„â€˜éº»è¾£ç¨‹åº¦â€™å’Œâ€˜ä¾¿å®œç¨‹åº¦â€™éƒ½å°½å¯èƒ½é«˜ã€‚**ä¸ºæ­¤ï¼ŒKey çš„ä¸¤ä¸ªç»´åº¦å¿…é¡»åˆ†åˆ«å®šä¹‰ä¸ºâ€˜è¶Šè¾£å€¼è¶Šå¤§â€™å’Œâ€˜è¶Šä¾¿å®œå€¼è¶Šå¤§â€™ã€‚â€** ç®€å•ç†è§£ï¼ŒQueryå°±æ˜¯åŠ æƒå¹³å‡çš„æƒé‡ã€‚

- **Key**: å‡è®¾æœ‰ä¸‰å®¶é¥­åº—ï¼Œæ¯å®¶é¥­åº—ä½¿ç”¨èœå“éº»è¾£ç¨‹åº¦å’Œä»·æ ¼ä½œä¸ºå…¶é¥­åº—ç‰¹å¾ï¼Œä¾‹å¦‚ Key = [0.8, 0.7], ç¬¬ä¸€ä¸ªç»´åº¦è¡¨ç¤ºé¥­åº—èœå“çš„éº»è¾£ç¨‹åº¦çš„å¾—åˆ†ï¼ˆè¶Šè¾£å¾—åˆ†è¶Šé«˜ï¼‰ï¼Œç¬¬äºŒä¸ªç»´åº¦è¡¨ç¤ºé¥­åº—ä»·æ ¼çš„ä¾¿å®œç¨‹åº¦ï¼ˆå€¼è¶Šæ¥è¿‘1ï¼Œè¡¨ç¤ºé¥­åº—è¶Šä¾¿å®œï¼‰ï¼Œè¿™ä¸ªè¦ä¸Query çš„ç†è§£ä¸€è‡´ã€‚

> ğŸ’¡ ç”¨å‡ ä¸ªä¸æ°å½“çš„ä¾‹å­ï¼ˆæœ‰ä¸ªäººä¸»è§‚å› ç´ ğŸ˜„ï¼‰ï¼š
> - ç²¤èœé¦†ï¼šKey = [0.1, 0.3]   ä¸è¾£ï¼Œä»·æ ¼è¾ƒé«˜ 
> - å·èœé¦†ï¼šKey = [0.9, 0.5]   éº»è¾£ï¼Œä»·æ ¼ä¸­ç­‰
> - æ—¥å¼é¤å…ï¼šKey = [0.3, 0.1] ä¸è¾£ï¼Œä»·æ ¼å¤ªè´µ

- **Value**: æ¯ä¸ªé¥­åº—èƒ½æä¾›çš„æœ‰ç”¨ä¿¡æ¯ï¼Œä¾‹å¦‚ Value = [0.9, 0.8] å¯ä»¥è¡¨ç¤ºç”¨æˆ·ç»¼åˆè¯„åˆ†0.9ï¼Œå‡ºé¤é€Ÿåº¦0.8ã€‚

> ğŸ’¡ Value ä¸éœ€è¦ä¸ Query å’Œ Key çš„ç»´åº¦æ•°é‡å’Œå«ä¹‰ä¿æŒä¸€è‡´ï¼ŒValueå¯ä»¥æ˜¯ä»»ä½•æœ‰ç”¨çš„ä¿¡æ¯ã€‚

**ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ä¸€ä¸‹åŸå§‹åŒ¹é…åˆ†æ•°**ï¼š

$$
\text{score}_i = \mathbf{q}^\top \mathbf{k}_i = 0.9 \cdot k_{i1} + 0.8 \cdot k_{i2}
$$

é€ä¸ªè®¡ç®—ï¼š

| é¥­åº—      | è®¡ç®—è¿‡ç¨‹                                        | åŸå§‹åˆ†æ•° |
| --------- | ----------------------------------------------- | -------- |
| Aï¼ˆç²¤èœï¼‰ | $0.9 \times 0.1 + 0.8 \times 0.3 = 0.09 + 0.24$ | **0.33** |
| Bï¼ˆå·èœï¼‰ | $0.9 \times 0.9 + 0.8 \times 0.5 = 0.81 + 0.40$ | **1.21** |
| Cï¼ˆæ—¥æ–™ï¼‰ | $0.9 \times 0.3 + 0.8 \times 0.1 = 0.27 + 0.08$ | **0.35** |

> ğŸ’¡ å·èœé¦†é¥é¥é¢†å…ˆâ€”â€”åˆè¾£åˆç›¸å¯¹ä¾¿å®œï¼

**ç¬¬äºŒæ­¥ï¼šSoftmax å½’ä¸€åŒ– â†’ æ³¨æ„åŠ›å¾—åˆ†ï¼ˆæƒé‡ï¼‰**

$$
\alpha_i = \frac{\exp(\text{score}_i)}{\exp(0.33) + \exp(1.21) + \exp(0.35)}
$$

å…ˆç®—æŒ‡æ•°ï¼ˆä½¿ç”¨è®¡ç®—å™¨ï¼Œä¿ç•™4ä½å°æ•°ï¼‰ï¼š

$$
\begin{aligned}
\exp(0.33) &\approx 1.3910 \\
\exp(1.21) &\approx 3.3535 \\
\exp(0.35) &\approx 1.4191 \\
\text{æ€»å’Œ} &= 1.3910 + 3.3535 + 1.4191 = 6.1636
\end{aligned}
$$

å†è®¡ç®—æ¯ä¸ªé¥­åº—çš„**æ³¨æ„åŠ›å¾—åˆ†ï¼ˆAttention Score / Weightï¼‰**ï¼š

| é¥­åº—      | å…¬å¼          | æ³¨æ„åŠ›å¾—åˆ†ï¼ˆÎ±áµ¢ï¼‰ | ç™¾åˆ†æ¯”    |
| --------- | ------------- | ---------------- | --------- |
| Aï¼ˆç²¤èœï¼‰ | 1.3910/6.1636 | **0.2257**       | **22.6%** |
| Bï¼ˆå·èœï¼‰ | 3.3535/6.1636 | **0.5441**       | **54.4%** |
| Cï¼ˆæ—¥æ–™ï¼‰ | 1.4191/6.1636 | **0.2302**       | **23.0%** |


- **å·èœé¦†è·å¾—è¶…è¿‡ä¸€åŠï¼ˆ54.4%ï¼‰çš„æ³¨æ„åŠ›**  
  â†’ å› ä¸ºå®ƒæœ€ç¬¦åˆä½ â€œåˆè¾£åˆä¾¿å®œâ€çš„åå¥½
- **ç²¤èœé¦†å’Œæ—¥æ–™é¦†å„å çº¦ 1/4**  
  â†’ è™½ç„¶éƒ½ä¸è¾£ï¼Œä½†ç²¤èœç¨ä¾¿å®œï¼ˆä¾¿å®œç¨‹åº¦ 0.3 > 0.1ï¼‰ï¼Œæ‰€ä»¥ç•¥é«˜äºæ—¥æ–™

> è¿™äº›æ³¨æ„åŠ›å¾—åˆ†å†³å®šäº†åç»­èšåˆ Value æ—¶çš„**è¯è¯­æƒ**ï¼š  
> å·èœé¦†çš„è¯„åˆ†å’Œå‡ºé¤é€Ÿåº¦å¯¹æœ€ç»ˆç»“æœå½±å“æœ€å¤§ï¼Œä¹‹åå°†æ³¨æ„åŠ›å¾—åˆ†ä¹˜ä»¥Valueå¹¶æ±‚å’Œåï¼Œå¾—åˆ°çš„æ˜¯ä¸€ä¸ªä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ¨èæ‘˜è¦ â€”â€” ä¸æ˜¯æŸä¸€å®¶é¥­åº—ï¼Œè€Œæ˜¯æ ¹æ®ä½ çš„åå¥½åŠ¨æ€èåˆä¸‰å®¶é¥­åº—çš„ç»¼åˆè¡¨ç¤ºï¼Œå¯ä»¥ç†è§£ä¸ºæ˜¯ä¸€ä¸ªè™šæ‹Ÿçš„é¥­åº—ï¼Œç”¨äºæŒ‡å¯¼åç»­çš„å†³ç­–ã€‚

æ¥ä¸‹æ¥æ€è€ƒï¼Œå¦‚æœæˆ‘çš„è€ƒè™‘ç»´åº¦å¾ˆå¤šå‘¢ï¼Ÿæ¯”å¦‚æˆ‘æ—¢å…³æ³¨ä»·æ ¼ï¼Œåˆå…³æ³¨éº»è¾£å£å‘³ï¼Œè¿˜å…³æ³¨å¥åº·ç­‰ç­‰ï¼Œæ˜¯å¦å¯ä»¥å°†è¿™äº›åå¥½ç»´åº¦éƒ½æ”¾è¿›Queryä¸­ï¼Ÿ

ç†è®ºä¸Šå¯ä»¥ï¼Œä½†æ˜¯å®è·µä¸Šçš„æ•ˆæœå¯èƒ½ä¸å¤ªå¥½ï¼Œå› ä¸ºç»´åº¦å¤ªå¤šï¼Œå¯èƒ½å¯¼è‡´ä¸åŒç»´åº¦çš„ç›¸äº’å¹²æ‰°ï¼Œæ‰€æœ‰çš„ç»´åº¦æ··åœ¨ä¸€èµ·è®¡ç®—æ€»çš„åˆ†æ•°ï¼Œå¯¼è‡´æœ€ç»ˆçš„å¾—åˆ†å¯èƒ½ä¸æ˜¯æœ€åˆé€‚çš„ã€‚

å¤šå¤´çš„ç›®çš„åœ¨è¿™é‡Œå°±ä½“ç°å‡ºæ¥äº†ï¼Œæˆ‘å¯ä»¥è®¾ç½®å¤šä¸ªå¤´ï¼Œæ¯ä¸ªå¤´éƒ½å…³æ³¨å‡ ä¸ªç»´åº¦ï¼Œç„¶åæŠŠå¤šä¸ªå¤´çš„ç»“æœè¿›è¡Œæ‹¼æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´é€‚åˆçš„è¡¨ç¤ºã€‚

è¿›ä¸€æ­¥è§£é‡Šï¼ˆæ¨ç†é˜¶æ®µçš„ï¼‰KVç¼“å­˜ï¼Œå› ä¸ºä¸ªäººçš„åå¥½ Query å¯èƒ½ä¼šå˜ï¼Œä½†æ˜¯é¥­åº—çš„ç‰¹å¾(Key/Value) ä¸ä¼šå˜ï¼Œæ‰€ä»¥åªè¦é¥­åº—æ²¡æœ‰æ¢èœå•ï¼Œæ²¡è°ƒä»·ï¼Œå®ƒä»¬çš„ Key å’Œ Value éƒ½ä¸ä¼šå˜ï¼Œæ‰€ä»¥å¯ä»¥ç¼“å­˜èµ·æ¥ï¼Œä¸‹æ¬¡æŸ¥è¯¢çš„æ—¶å€™ç›´æ¥ä»ç¼“å­˜ä¸­å–ï¼Œçœå¾—é‡å¤è®¡ç®—ã€‚ï¼ˆè¿™ä¸ªä¾‹å­å®é™…ä¸Šå¯èƒ½ä¸å¤ªæ°å½“ï¼Œä½†æ˜¯å‹‰å¼ºå¯ä»¥ç†è§£ï¼‰

### 7.2 æ³¨æ„åŠ›æœºåˆ¶å®ç°

å›åˆ°miniGPTçš„å®ç°ä¸Šï¼Œæˆ‘ä»¬æŠŠæ³¨æ„åŠ›æœºåˆ¶çš„å…¬å¼å†™ä¸€ä¸‹ï¼š
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

è¿™é‡Œéœ€è¦æ³¨æ„ï¼Œæˆ‘ä»¬è¦åœ¨è¿™ä¸ªå…¬å¼çš„åŸºç¡€ä¸Šï¼Œå¢åŠ æ³¨æ„åŠ›æ©ç ï¼Œç”±äºè¾“å…¥åºåˆ—å¯èƒ½æ˜¯ä¸åŒçš„é•¿åº¦ï¼Œä½†çŸ©é˜µè¿ç®—æ—¶éœ€è¦å›ºå®šçš„å¤§å°ï¼Œå› æ­¤é’ˆå¯¹é•¿åº¦ä¸è¶³çš„åºåˆ—ï¼Œä½¿ç”¨paddingä½œå¡«å……ï¼Œä½†æ˜¯è¿™äº›paddingçš„ä¿¡æ¯æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› æ­¤éœ€è¦å°†è¿™äº›paddingçš„ä½ç½®çš„æ³¨æ„åŠ›æ©ç è®¾ç½®ä¸º0ï¼Œè¿™æ ·åœ¨è®¡ç®—softmaxçš„æ—¶å€™ï¼Œè¿™äº›ä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°å°±ä¼šå˜æˆ0ï¼Œä»è€Œè¢«å¿½ç•¥ã€‚


```python
import torch
import torch.nn as nn
import numpy as np

class ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k):
        super(ScaledDotProductAttention, self).__init__()
        self.d_k = d_k  # è¾“å…¥çš„ç»´åº¦ï¼Œç”¨äºè®¡ç®—ç¼©æ”¾å› å­

    def forward(self, q, k, v, attn_mask):
        """
        Args:
            q: [batch_size, n_heads, len_q, d_k]
            k: [batch_size, n_heads, len_k, d_k]
            v: [batch_size, n_heads, len_v, d_v]  (æ³¨æ„: len_k == len_v)
            attn_mask: [batch_size, n_heads, len_q, len_k] æˆ– None
        Returns:
            context: [batch_size, n_heads, len_q, d_v]
            attn:    [batch_size, n_heads, len_q, len_k]
        """
        # step 1: è®¡ç®— QK^T / sqrt(d_k) shape: [batch_size, n_heads, len_q, len_k]
        scores = torch.matmul(q, k.transpose(-1, -2)) / np.sqrt(self.d_k)

        # step 2: åº”ç”¨ attn_mask
        if attn_mask is not None:
            # masked_fill_: å°† mask=False çš„ä½ç½®æ›¿æ¢ä¸º -1e9
            # è¿™é‡Œå¦‚æœæ›¿æ¢ä¸º0ï¼Œåé¢çš„softmaxè®¡ç®—æ—¶å€™ï¼Œå¦‚æœå…¶ä»–é¡¹çš„å€¼ä¹Ÿæ¯”è¾ƒå°
            # é‚£ä¹ˆsoftmaxä¹‹åçš„ç»“æœå¯èƒ½è¿˜æœ‰æƒé‡ï¼Œæ‰€ä»¥è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼Œå¯ä»¥é¿å…è¿™ç§æƒ…å†µ
            scores = scores.masked_fill(~attn_mask, -1e9)

        # step 3: softmax å¾—åˆ° attn
        attn = nn.Softmax(dim=-1)(scores)  # shape: [batch_size, n_heads, len_q, len_k]

        # step 4: attn * v
        context = torch.matmul(attn, v)  # shape: [batch_size, n_heads, len_q, head_dim]

        return context, attn


if __name__ == "__main__":
    # ç”¨æˆ·åå¥½ Query
    q = torch.tensor([[[[0.9, 0.8]]]])  # shape: [1, 1, 1, 2]

    # ä¸‰å®¶é¥­åº—çš„ Key: [éº»è¾£ç¨‹åº¦, ä¾¿å®œç¨‹åº¦]
    k = torch.tensor(
        [[[[0.1, 0.3], [0.9, 0.5], [0.3, 0.1]]]]  # ç²¤èœé¦†  # å·èœé¦†  # æ—¥å¼é¤å…
    )  # shape: [1, 1, 3, 2]

    # ä¸‰å®¶é¥­åº—çš„ Value: [è¯„åˆ†, å‡ºé¤é€Ÿåº¦]
    v = torch.tensor(
        [[[[0.5, 0.9], [0.9, 0.6], [0.7, 0.8]]]]  # ç²¤èœé¦†  # å·èœé¦†  # æ—¥å¼é¤å…
    )  # shape: [1, 1, 3, 2]

    # æ²¡æœ‰mask
    mask = None

    attention = ScaledDotProductAttention(d_k=2)

    context, attn_weights = attention(q, k, v, mask)

    print("=== Attention Weights (Î±) ===")
    print(attn_weights.squeeze())  # shape: [3]

    print("\n=== Output Context (Weighted Value) ===")
    print(context.squeeze())  # shape: [2]
```

è¿›ä¸€æ­¥éªŒè¯ä¸€ä¸‹ï¼Œæˆ‘ä¸å–œæ¬¢æ—¥å¼é¤å…ï¼Œç›´æ¥å±è”½ `mask[:,:,:,2] = True`

```python
mask = torch.ones(1, 1, 1, 3, dtype=torch.bool)
mask[:, :, :, 2] = False
```

### 7.3 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°
åœ¨ä¸Šé¢çš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
```python {5-79}
import torch
import torch.nn as nn
import numpy as np

class MultiHeadAttention(nn.Module):
    def __init__(self, n_heads, d_model, d_k, d_v):
        super(MultiHeadAttention, self).__init__()
        self.n_heads = n_heads
        self.d_model = d_model
        self.d_k = d_k
        self.d_v = d_v

        # çº¿æ€§å˜æ¢å±‚
        self.w_q = nn.Linear(d_model, n_heads * d_k, bias=False)
        self.w_k = nn.Linear(d_model, n_heads * d_k, bias=False)
        self.w_v = nn.Linear(d_model, n_heads * d_v, bias=False)

        # è¾“å‡ºå±‚
        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)

        # æ³¨æ„åŠ›æ¨¡å—
        self.attention = ScaledDotProductAttention(d_k)

        # LayerNormå±‚
        self.layer_norm = nn.LayerNorm(d_model)

    def forward(self, q, k, v, attn_mask=None):
        """
        Args:
            q: [batch_size, len_q, d_model]
            k: [batch_size, len_k, d_model]
            v: [batch_size, len_v, d_model]  (len_k == len_v)
            attention_mask: [batch_size, len_q, len_k] or None
        Returns:
            output: [batch_size, len_q, d_model]
            attn:   [batch_size, n_heads, len_q, len_k]
        """
        residual = q  # æ®‹å·®è¿æ¥
        batch_size = q.size(0)

        # å°†qã€kã€vè¿›è¡Œçº¿æ€§æ˜ å°„æ‹†åˆ†
        # [Batch_size, len_q, d_model] -> [Batch_size, len_q, n_heads, d_k]
        q_proj = self.w_q(q).view(
            batch_size, -1, self.n_heads, self.d_k
        )  # [batch_size, len_q, n_heads, d_k]
        k_proj = self.w_k(k).view(
            batch_size, -1, self.n_heads, self.d_k
        )  # [batch_size, len_k, n_heads, d_k]
        v_proj = self.w_v(v).view(
            batch_size, -1, self.n_heads, self.d_v
        )  # [batch_size, len_v, n_heads, d_v]

        # Transpose to [B, n_heads, Len_v, d_v]
        q_proj = q_proj.transpose(1, 2)
        k_proj = k_proj.transpose(1, 2)
        v_proj = v_proj.transpose(1, 2)

        if attn_mask is not None:  # å¦‚æœattn_maskä¸ä¸ºNoneï¼Œåˆ™è¿›è¡Œmaskæ“ä½œ
            # [B, Lq, Lk] -> [B, 1, Lq, Lk] -> [B, n_heads, Lq, Lk]
            attn_mask = attn_mask.unsqueeze(1).expand(-1, self.n_heads, -1, -1)
        else:
            attn_mask = None

        # scaled dot-product attention
        context, attn = self.attention(q_proj, k_proj, v_proj, attn_mask)

        # concat heads
        # [batch_size, n_heads, len_q, d_v] -> [batch_size, len_q, n_heads * d_v]
        context = (
            context.transpose(1, 2)
            .contiguous()
            .view(batch_size, -1, self.n_heads * self.d_v)
        )

        output = self.fc(context)

        output = self.layer_norm(output + residual)

        return output, attn


if __name__ == "__main__":
    # æ¨¡æ‹Ÿä¸‰å®¶é¥­åº—çš„åŸå§‹ç‰¹å¾ [è¾£, ä¾¿å®œ, å¥åº·, è¯„åˆ†]
    # shape: [batch=1, seq_len=3, d_model=4]
    k_v_input = torch.tensor(
        [
            [
                [0.1, 0.3, 0.8, 0.5],  # ç²¤èœé¦†
                [0.9, 0.5, 0.2, 0.9],  # å·èœé¦†
                [0.3, 0.1, 0.9, 0.7],  # æ—¥å¼é¤å…
            ]
        ]
    )

    # Queryï¼šå½“å‰ç”¨æˆ·åå¥½ï¼ˆä¹Ÿç”¨åŒæ ·4ç»´è¡¨ç¤ºï¼‰
    q_input = torch.tensor([[[0.9, 0.8, 0.3, 0.6]]])  # æˆ‘å–œæ¬¢è¾£ã€ä¾¿å®œï¼Œä¸å¤ªåœ¨æ„å¥åº·

    # æµ‹è¯•1: æ—  mask
    mha = MultiHeadAttention(n_heads=2, d_model=4, d_k=2, d_v=2)
    output, attn_weights = mha(q_input, k_v_input, k_v_input, attn_mask=None)
    print("=== No Mask ===")
    print("Output shape:", output.shape)  # [1, 1, 4]
    print("Attn shape:", attn_weights.shape)  # [1, 2, 1, 3]
    print("Head 1 weights:", attn_weights[0, 0, 0].tolist())
    print("Head 2 weights:", attn_weights[0, 1, 0].tolist())

    # æµ‹è¯•2: å±è”½æ—¥å¼é¤å… (index=2)
    mask = torch.ones(1, 1, 3, dtype=torch.bool)
    mask[0, 0, 2] = False  # False = å±è”½
    output2, attn_weights2 = mha(q_input, k_v_input, k_v_input, attn_mask=mask)
    print("\n=== With Mask ===")
    print("Head 1 weights:", attn_weights2[0, 0, 0].tolist())
    print("Head 2 weights:", attn_weights2[0, 1, 0].tolist())
```

### 7.4 æ”¹é€ æˆ CasualSelfAttention
æˆ‘ä»¬æŠ›å¼ƒæ‰è‡ªå·±åˆ›å»ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œæ”¹ç”¨ PyTorch æä¾›çš„å®ç°ï¼ŒåŒæ—¶æŠŠæ•´ä½“çš„ä»£ç é€‚é… miniGPT çš„æ¨¡å‹ä¸Šã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional
from dataclasses import dataclass


@dataclass
class GPTConfig:
    """
    æ¨¡å‹è¶…å‚æ•°é…ç½®ç±»ï¼ˆä½¿ç”¨ dataclass ç®€æ´å®šä¹‰ï¼‰

    é»˜è®¤å€¼å‚è€ƒ GPT-2 smallï¼Œä½† vocab_size æ ¹æ®ä¸­æ–‡ä»»åŠ¡è°ƒæ•´
    """

    n_embd: int = 768  # token embedding ç»´åº¦ï¼ˆä¹Ÿå³ hidden sizeï¼‰
    n_head: int = 8  # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¿…é¡»æ•´é™¤ n_embdï¼‰
    n_layer: int = 6  # Transformer block å±‚æ•°
    dropout: float = 0.1  # æ‰€æœ‰ dropout å±‚çš„ä¸¢å¼ƒç‡
    block_size: int = 256  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä½ç½®ç¼–ç æœ€å¤§æ”¯æŒé•¿åº¦ï¼‰
    vocab_size: int = 4825  # è¯è¡¨å¤§å°ï¼ˆæ ¹æ®å®é™… tokenizer å†³å®š


class CausalSelfAttention(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()

        assert config.n_embd % config.n_head == 0
        self.config = config
        # å°† embedding åˆ†æˆäº†å¤šä¸ª head
        self.head_dim = config.n_embd // config.n_head
        # åˆ›å»º queryã€keyã€value æŠ•å½±
        # å°† [B, T, C] ä¸€æ¬¡æ€§æŠ•å½±ä¸º [B, T, 3 * C]ï¼Œç„¶åå†æ‹†åˆ†ä¸º Q/K/V
        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd)

        # è¾“å‡ºæŠ•å½±ï¼šå°†å¤šå¤´æ‹¼æ¥åçš„å‘é‡æ˜ å°„å›åŸå§‹ç»´åº¦
        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)

        # æ³¨æ„åŠ›è¾“å‡ºçš„æ®‹å·®è¿æ¥ååŠ  dropout
        self.resid_dropout = nn.Dropout(config.dropout)

    def forward(
        self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, n_embd]
            attention_mask: å¯é€‰ï¼Œpadding æ©ç ï¼Œå½¢çŠ¶ [batch_size, seq_len]
                - 1 è¡¨ç¤ºæœ‰æ•ˆ token
                - 0 è¡¨ç¤º padding token

        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, n_embd]
        """
        # batch, sequence length, embedding dim
        B, T, C = x.shape

        # === æ­¥éª¤1: QKV èåˆæŠ•å½± ===
        qkv = self.qkv_proj(x)
        # æ‹†åˆ†æˆ Q, K, V, ç»´åº¦å‡æ˜¯ [B, T, C]
        q, k, v = qkv.chunk(3, dim=-1)

        # === æ­¥éª¤2: å¤šå¤´åˆ†å‰²ä¸è½¬ç½® ===
        # å°†æ¯ä¸ªå¤´çš„ç»´åº¦ä»Cæ‹†ä¸º n_head, head_dim
        # ç„¶åè½¬ç½®ä¸º [B, n_head, T, head_dim] ä»¥ä¾¿è¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜
        q = q.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)

        # === æ­¥éª¤3: æ„å»ºå¤åˆæ³¨æ„åŠ›æ©ç  ===

        attn_mask = None
        if attention_mask is not None:
            # padding æ©ç : [B, T] -> bool
            key_padding_mask = attention_mask.to(torch.bool)

            # å› æœæ©ç ï¼šä¸‹ä¸‰è§’çŸ©é˜µ[T, T]ï¼Œç¡®ä¿åªèƒ½ attend åˆ°å½“å‰åŠå·¦ä¾§
            causal_mask = torch.tril(
                torch.ones(T, T, dtype=torch.bool, device=x.device)
            )

            # æ‹“å±•ç»´åº¦ä»¥æ”¯æŒå¹¿æ’­
            # key_padding_mask: [B, 1, 1, T]
            # causal_mask: [1, 1, T, T]
            key_padding_mask = key_padding_mask[:, None, None, :]
            causal_mask = causal_mask[None, None, :, :]

            # é€»è¾‘ä¸ï¼šåªæœ‰åŒæ—¶æ»¡è¶³â€œé paddingâ€å’Œâ€œåœ¨å·¦ä¾§â€æ‰ä¸º True
            attn_mask = causal_mask & key_padding_mask  # [B, 1, T, T]

        # === æ­¥éª¤4: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®— ===
        y = F.scaled_dot_product_attention(
            q,
            k,
            v,
            attn_mask=attn_mask,
            dropout_p=self.config.dropout if self.training else 0,
            is_causal=False,  # NOTEï¼šæˆ‘ä»¬å·²æ‰‹åŠ¨æ„å»ºå› æœæ©ç ï¼Œæ•…è®¾ä¸º False
        )

        # è¾“å‡º y: [B, n_head, T, head_dim]

        # === æ­¥éª¤5: åˆå¹¶å¤šå¤´å¹¶æŠ•å½± ===
        # è½¬ç½®å› [B, T, n_head, head_dim] -> åˆå¹¶æœ€åä¸¤ç»´ -> [B, T, C]
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        # ç»è¿‡è¾“å‡ºæŠ•å½± + æ®‹å·® dropout
        y = self.resid_dropout(self.out_proj(y))
        return y


# ========================
# æµ‹è¯•ä»£ç 
# ========================
if __name__ == "__main__":
    torch.manual_seed(42)

    # é…ç½®
    config = GPTConfig(n_embd=8, n_head=2, dropout=0.0)

    # åˆ›å»ºæ¨¡å—
    attn = CausalSelfAttention(config)

    # è¾“å…¥ï¼šbatch=2, seq_len=5, emb=8
    x = torch.randn(2, 5, 8)

    # attention_mask: ç¬¬ä¸€ä¸ªæ ·æœ¬æœ‰æ•ˆé•¿åº¦=3ï¼Œç¬¬äºŒä¸ª=4
    attention_mask = torch.tensor(
        [[1, 1, 1, 0, 0], [1, 1, 1, 1, 0]]
    )  # 1=æœ‰æ•ˆ, 0=padding

    # å‰å‘ä¼ æ’­
    output = attn(x, attention_mask=attention_mask)

    print("Input shape:", x.shape)
    print("Output shape:", output.shape)
    print("Output (first sample, first token):", output[0, 0].detach().numpy())

    # éªŒè¯å› æœæ€§ï¼šæ£€æŸ¥æ˜¯å¦åª attend åˆ°å·¦ä¾§
    # æ‰‹åŠ¨è®¡ç®— attention weightsï¼ˆä»…ç”¨äºéªŒè¯ï¼Œéå¿…éœ€ï¼‰
    with torch.no_grad():
        qkv = attn.qkv_proj(x)
        q, k, _ = qkv.chunk(3, dim=-1)
        q = q.view(2, 5, 2, 4).transpose(1, 2)
        k = k.view(2, 5, 2, 4).transpose(1, 2)
        scores = torch.matmul(q, k.transpose(-2, -1)) / (4**0.5)  # [2, 2, 5, 5]
        # åº”ç”¨å› æœæ©ç ï¼ˆä¸‹ä¸‰è§’ï¼‰
        causal_mask = torch.tril(torch.ones(5, 5, dtype=torch.bool))
        scores_masked = scores.masked_fill(~causal_mask, float("-inf"))
        attn_weights = F.softmax(scores_masked, dim=-1)
        print("\nAttention weights for head 0, sample 0:")
        print(attn_weights[0, 0].numpy())
        # åº”è¯¥æ˜¯ä¸‹ä¸‰è§’ï¼Œä¸”æ¯è¡Œå’Œä¸º1
```

### 7.5 æ„å»ºå®Œæ•´çš„GPTæ¨¡å‹
å‰©ä¸‹çš„æ¨¡å‹ç»“æ„ç›¸å¯¹ç®€å•ï¼Œè¿™é‡Œå°±ä¸ä¸“é—¨ä»‹ç»äº†ï¼Œæˆ‘ä»¬ç›´æ¥æ„å»ºæ•´ä½“çš„GPTæ¨¡å‹ã€‚

**å‰é¦ˆç¥ç»ç½‘ç»œMLPå±‚**
```python
class MLP(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰

    GPT æ ‡å‡†ç»“æ„ï¼š
      Linear(n_embd, 4*n_embd) â†’ GELU â†’ Linear(4*n_embd, n_embd)

    ä¸ºä»€ä¹ˆæ˜¯ 4 å€ï¼Ÿâ€”â€” GPT è®ºæ–‡å‘ç°æ­¤æ¯”ä¾‹åœ¨æ€§èƒ½ä¸è®¡ç®—é—´å–å¾—è‰¯å¥½å¹³è¡¡
    """

    def __init__(self, config: GPTConfig):
        super().__init__()

        # å‡ç»´ï¼šæ‰©å¤§è¡¨ç¤ºèƒ½åŠ›
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        # é™ç»´ï¼šå›åˆ°åŸå§‹ç»´åº¦
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        # è¾“å‡º dropout
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.gelu(x)  # GELU æ¿€æ´»å‡½æ•°ï¼ˆGPT ç³»åˆ—æ ‡å‡†é€‰æ‹©ï¼‰
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```

**Transformer è§£ç å™¨å—**
```python
class Block(nn.Module):
    """
    Transformer è§£ç å™¨å—ï¼ˆDecoder Blockï¼‰

    ç»“æ„ï¼ˆPre-LN é£æ ¼ï¼‰ï¼š
      x â†’ LayerNorm â†’ Attention â†’ Add â†’ LayerNorm â†’ MLP â†’ Add â†’ Output

    Pre-LN ä¼˜åŠ¿ï¼š
      - è®­ç»ƒæ›´ç¨³å®šï¼ˆæ¢¯åº¦ä¸ä¼šéšå±‚æ•°çˆ†ç‚¸ï¼‰
      - æ— éœ€å­¦ä¹ ç‡ warmupï¼ˆåœ¨ä¸­å°æ¨¡å‹ä¸­æ•ˆæœæ˜¾è‘—ï¼‰
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        # ç¬¬ä¸€ä¸ª LayerNormï¼ˆç”¨äº Attention å‰ï¼‰
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        # ç¬¬äºŒä¸ª LayerNormï¼ˆç”¨äº MLP å‰ï¼‰
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(
        self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        # æ³¨æ„åŠ›å­å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        x = x + self.attn(self.ln_1(x), attention_mask)
        # MLP å­å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        x = x + self.mlp(self.ln_2(x))
        return x
```

**Transformer ç»“æ„**
```python
class Transformer(nn.Module):
    """
    åŒ…å«ï¼š
      - Token Embedding
      - å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆLearned Positional Embeddingï¼‰
      - N ä¸ª Block
      - æœ€ç»ˆ LayerNormï¼ˆPre-LN ç»“æ„çš„ä¸€éƒ¨åˆ†ï¼‰
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config

        # Token Embedding: [vocab_size, n_embd]
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)

        # ä½ç½®ç¼–ç : [block_size, n_embd]ï¼ˆå¯å­¦ä¹ ï¼Œéæ­£å¼¦ï¼‰
        # æ³¨æ„ï¼šGPT-2 ä½¿ç”¨å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼Œè€ŒéåŸå§‹ Transformer çš„å›ºå®šç¼–ç 
        self.wpe = nn.Embedding(config.block_size, config.n_embd)

        # è¾“å…¥ dropout
        self.drop = nn.Dropout(config.dropout)

        # å †å  N ä¸ª Transformer Block
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])

        # æœ€ç»ˆ LayerNormï¼ˆPre-LN ç»“æ„è¦æ±‚åœ¨æœ€ååŠ ä¸€æ¬¡ LNï¼‰
        self.ln_f = nn.LayerNorm(config.n_embd)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            input_ids: [B, T]ï¼Œtoken ID åºåˆ—
            attention_mask: [B, T]ï¼Œå¯é€‰ padding æ©ç 

        Returns:
            hidden_states: [B, T, n_embd]
        """
        B, T = input_ids.shape
        # å®‰å…¨æ£€æŸ¥ï¼šåºåˆ—é•¿åº¦ä¸èƒ½è¶…è¿‡ block_size
        assert (
            T <= self.config.block_size
        ), f"åºåˆ—é•¿åº¦ {T} è¶…å‡ºæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ {self.config.block_size}"

        # === è·å– token embedding ===
        tok_emb = self.wte(input_ids)  # [B, T, n_embd]

        # === è·å–ä½ç½® embedding ===
        # ä½¿ç”¨ arange ç”Ÿæˆä½ç½®ç´¢å¼• [0, 1, 2, ..., T-1]
        pos = torch.arange(0, T, dtype=torch.long, device=input_ids.device)
        pos_emb = self.wpe(pos)  # [T, n_embd]

        # === åˆå¹¶ token + ä½ç½® embedding ===
        x = self.drop(tok_emb + pos_emb)  # [B, T, n_embd]

        # === é€šè¿‡æ‰€æœ‰ Transformer Block ===
        for block in self.blocks:
            x = block(x, attention_mask)

        # === æœ€ç»ˆ LayerNorm ===
        x = self.ln_f(x)
        return x
```

**å®Œæ•´çš„GPTæ¨¡å‹**
```python
class GPTLMHeadModel(nn.Module):
    """
    å®Œæ•´çš„ GPT è¯­è¨€æ¨¡å‹ï¼ˆå«è¯­è¨€å»ºæ¨¡å¤´ï¼‰

    å…³é”®ç‰¹æ€§ï¼š
      - æƒé‡ç»‘å®šï¼ˆWeight Tyingï¼‰ï¼šwte.weight = lm_head.weight
        * å‡å°‘å‚æ•°é‡
        * æå‡è®­ç»ƒç¨³å®šæ€§ï¼ˆPress & Wolf, 2017ï¼‰
      - è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config
        self.transformer = Transformer(config)

        # è¯­è¨€å»ºæ¨¡å¤´ï¼šå°† hidden state æ˜ å°„åˆ° vocab logits
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # ğŸ”‘ æƒé‡ç»‘å®šï¼šå…±äº« embedding ä¸ lm_head çš„æƒé‡çŸ©é˜µ
        # æ³¨æ„ï¼šå¿…é¡»åœ¨åˆå§‹åŒ–åç«‹å³ç»‘å®šï¼Œä¸”ä¸¤è€… bias å‡ä¸º False
        self.transformer.wte.weight = self.lm_head.weight

        # åˆå§‹åŒ–æ‰€æœ‰å‚æ•°
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """
        å‚æ•°åˆå§‹åŒ–ç­–ç•¥ï¼ˆéµå¾ª GPT-2ï¼‰
        - Linear / Embedding: Normal(0, 0.02)
        - Bias: å…¨é›¶ï¼ˆä½†æœ¬æ¨¡å‹æ—  biasï¼‰
        """
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥ token IDsï¼Œè¾“å‡ºæ¯ä¸ªä½ç½®çš„è¯æ±‡è¡¨ logits

        Args:
            input_ids: [B, T]ï¼Œè¾“å…¥ token ID åºåˆ—
            attention_mask: [B, T]ï¼Œå¯é€‰ï¼Œç”¨äºå±è”½ padding

        Returns:
            logits: [B, T, vocab_size]ï¼Œæ¯ä¸ªä½ç½®å¯¹æ•´ä¸ªè¯è¡¨çš„é¢„æµ‹åˆ†æ•°
        """
        # é€šè¿‡ Transformer ç¼–ç å™¨è·å–éšè—çŠ¶æ€
        hidden_states = self.transformer(input_ids, attention_mask)

        # é€šè¿‡è¯­è¨€å»ºæ¨¡å¤´å¾—åˆ° logits
        logits = self.lm_head(hidden_states)  # [B, T, vocab_size]

        return logits
```

æ„å»ºå¥½åï¼Œæˆ‘ä»¬çœ‹ä¸€ä¸‹æ¨¡å‹çš„æ•´ä½“æƒ…å†µï¼š
```python
GPTLMHeadModel(
  (transformer): Transformer(
    (wte): Embedding(4825, 768)
    (wpe): Embedding(256, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-5): 6 x Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): CausalSelfAttention(
          (qkv_proj): Linear(in_features=768, out_features=2304, bias=True)
          (out_proj): Linear(in_features=768, out_features=768, bias=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          (c_fc): Linear(in_features=768, out_features=3072, bias=False)
          (c_proj): Linear(in_features=3072, out_features=768, bias=False)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=4825, bias=False)
)
```
æ•´ä½“çš„å‚æ•°é‡æ˜¯ ~46.4Mï¼Œç›¸æ¯”äº GPT-2ï¼ˆ124Mï¼‰ è¿˜æ˜¯å°å¾ˆå¤šã€‚

## 8. GPT æ¨¡å‹è®­ç»ƒ
è¿™éƒ¨åˆ†ä»…åšåŸºç¡€è®­ç»ƒï¼Œå› æ­¤æ²¡æœ‰è¿›è¡Œå¤§é‡ä¼˜åŒ–ã€‚

```python
import torch
from minigpt.qa_dataset import QADataset
from minigpt.tokenizer import Tokenizer
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torch.nn as nn
import os
from minigpt.model import GPTLMHeadModel, GPTConfig
from tqdm import tqdm


def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    num_batches = len(train_loader)

    progress_bar = tqdm(
        enumerate(train_loader),
        total=num_batches,
        desc="  Train",
        leave=False,
        unit="batch",
    )

    for batch_idx, batch in progress_bar:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)

        optimizer.zero_grad()
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / num_batches


@torch.no_grad()
def validate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    num_batches = len(val_loader)

    progress_bar = tqdm(
        enumerate(val_loader),
        total=num_batches,
        desc="  Val",
        leave=False,
        unit="batch",
    )

    for batch_idx, batch in progress_bar:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)

        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))
        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / num_batches


def save_checkpoint(model, optimizer, epoch, path):
    torch.save(
        {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        },
        path,
    )


def train_model(
    model,
    train_loader,
    val_loader,
    optimizer,
    criterion,
    device,
    num_epochs,
    model_output_dir,
    writer,
):
    os.makedirs(model_output_dir, exist_ok=True)
    best_val_loss = float("inf")

    for epoch in range(1, num_epochs + 1):
        print(f"\nEpoch {epoch}/{num_epochs}")

        # Training
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        print(f"  Train Loss: {train_loss:.4f}")

        # Validation
        val_loss = validate(model, val_loader, criterion, device)
        print(f"  Val Loss:   {val_loss:.4f}")

        # Log to TensorBoard
        if writer is not None:
            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Loss/val", val_loss, epoch)

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_path = os.path.join(model_output_dir, "best_model.pth")
            save_checkpoint(model, optimizer, epoch, best_path)
            print(f"  ğŸ‰ New best model saved (val loss: {val_loss:.4f})")

    print(f"\nâœ… Training finished. Best validation loss: {best_val_loss:.4f}")


def main():
    # é…ç½®è·¯å¾„
    train_path = "./data/train.jsonl"
    val_path = "./data/val.jsonl"
    vocab_path = "./data/vocab.json"

    # è¶…å‚æ•°
    max_length = 128
    batch_size = 64
    lr = 1e-4
    epochs = 15

    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    if device.type == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    # åŠ è½½ tokenizer å’Œæ¨¡å‹
    tokenizer = Tokenizer(vocab_path)
    config = GPTConfig(vocab_size=tokenizer.get_vocab_size())
    model = GPTLMHeadModel(config).to(device)

    # æ•°æ®é›†
    train_dataset = QADataset(train_path, tokenizer, max_length)
    val_dataset = QADataset(val_path, tokenizer, max_length)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        drop_last=True,
    )

    # ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # TensorBoard æ—¥å¿—
    writer = SummaryWriter("runs/minigpt")

    # å¼€å§‹è®­ç»ƒ
    train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        num_epochs=epochs,
        model_output_dir="output",
        writer=writer,
    )

    writer.close()
    print("\nğŸ‰ Training pipeline completed.")


if __name__ == "__main__":
    main()
```

> ğŸ’¡ åœ¨ä¸Šé¢çš„batch_size

## 9. æ¨¡å‹æ¨ç†
### 9.1 ä¸ºGPTæ·»åŠ generateæ–¹æ³•
ä¸ºäº†æ›´åŠ æ–¹ä¾¿ä½¿ç”¨ï¼Œä¸”ç¬¦åˆ Huggingface Transformer åº“çš„æ ‡å‡†èŒƒå¼ï¼Œè¿™é‡Œå°† `generate` æ–¹æ³•è¡¥å……æ·»åŠ åˆ° `GPTLMHeadModel` æ¨¡å‹ä¸­ã€‚

```python
def generate(
    self,
    input_ids: torch.Tensor,
    max_new_tokens: int = 20,
    stop_token_ids: Optional[Union[int, List[int]]] = None,
    temperature: float = 1.0,
    top_k: Optional[int] = None,
    do_sample: bool = False,
) -> torch.Tensor:
    """
    è‡ªå›å½’æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ï¼ˆæ”¯æŒå¤šç§è§£ç ç­–ç•¥ï¼‰

    Args:
        input_ids (torch.Tensor):
            åˆå§‹è¾“å…¥ token IDsï¼Œå½¢çŠ¶ [batch_size, seq_len]
        max_new_tokens (int):
            æœ€å¤šç”Ÿæˆçš„æ–° token æ•°é‡
        stop_token_ids (int or List[int], optional):
            é‡åˆ°è¿™äº› token æ—¶æå‰åœæ­¢ç”Ÿæˆï¼ˆå¦‚ <eos>ï¼‰ã€‚æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹åˆ¤æ–­æ˜¯å¦åœæ­¢ã€‚
        temperature (float):
            é‡‡æ ·æ¸©åº¦ï¼ˆ>1 æ›´éšæœºï¼Œ<1 æ›´ç¡®å®šï¼‰ï¼Œä»…åœ¨ do_sample=True æ—¶ç”Ÿæ•ˆ
        top_k (int, optional):
            é™åˆ¶é‡‡æ ·åªåœ¨æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­è¿›è¡Œ
        do_sample (bool):
            æ˜¯å¦ä½¿ç”¨éšæœºé‡‡æ ·ï¼ˆFalse è¡¨ç¤º greedy è§£ç ï¼‰

    Returns:
        generated_ids (torch.Tensor):
            å®Œæ•´ç”Ÿæˆåºåˆ—ï¼Œå½¢çŠ¶ [batch_size, seq_len + actual_new_tokens]
            æ³¨æ„ï¼šä¸åŒæ ·æœ¬å¯èƒ½ç”Ÿæˆä¸åŒé•¿åº¦ï¼Œä½†è¿”å›å¼ é‡æ˜¯ç»Ÿä¸€å³å¡«å……ï¼ˆç”¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token å¡«å……ï¼‰ï¼Œ
            è‹¥éœ€ä¸¥æ ¼æˆªæ–­ï¼Œè¯·åœ¨è°ƒç”¨åæŒ‰ stop token æ‰‹åŠ¨å¤„ç†ã€‚
    """
    self.eval()
    device = input_ids.device
    B, T = input_ids.shape

    # === å¤„ç†åœæ­¢ token ===
    stop_tokens = set()
    if stop_token_ids is not None:
        if isinstance(stop_token_ids, int):
            stop_tokens.add(stop_token_ids)
        else:
            stop_tokens.update(stop_token_ids)

    # è½¬ä¸º GPU tensor ç”¨äºå‘é‡åŒ–æ¯”è¾ƒï¼ˆé¿å… .item() åŒæ­¥ï¼‰
    stop_tensor = None
    if stop_tokens:
        stop_tensor = torch.tensor(list(stop_tokens), device=device, dtype=input_ids.dtype)  # [S]

    # === åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€ ===
    generated = input_ids.clone()  # [B, T]
    finished = torch.zeros(B, dtype=torch.bool, device=device)  # [B]ï¼Œè®°å½•æ¯ä¸ªæ ·æœ¬æ˜¯å¦å·²å®Œæˆ

    with torch.no_grad():
        for _ in range(max_new_tokens):
            # æå‰ç»ˆæ­¢ï¼šæ‰€æœ‰æ ·æœ¬éƒ½å·²å®Œæˆ æˆ– è¶…å‡ºä¸Šä¸‹æ–‡çª—å£
            if finished.all() or generated.size(1) >= self.config.block_size:
                break

            # è·å–å½“å‰ logitsï¼ˆåªå–æœ€åä¸€ä¸ªä½ç½®ï¼‰
            logits = self(generated)  # [B, T_curr, vocab_size]
            next_token_logits = logits[:, -1, :]  # [B, vocab_size]

            # === è§£ç ç­–ç•¥ ===
            if do_sample:
                # æ¸©åº¦ç¼©æ”¾ï¼ˆç¡®ä¿ temperature > 0ï¼‰
                if temperature <= 0:
                    raise ValueError("temperature must be > 0")
                next_token_logits = next_token_logits / temperature

                # Top-k è¿‡æ»¤
                if top_k is not None and top_k > 0:
                    k = min(top_k, next_token_logits.size(-1))
                    # è·å–ç¬¬ k å¤§çš„å€¼ä½œä¸ºé˜ˆå€¼
                    values, _ = torch.topk(next_token_logits, k, dim=-1)
                    threshold = values[:, -1:]  # [B, 1]
                    # å°†ä½äºé˜ˆå€¼çš„ logits è®¾ä¸º -inf
                    next_token_logits = torch.where(
                        next_token_logits < threshold,
                        torch.full_like(next_token_logits, float('-inf')),
                        next_token_logits
                    )

                # é‡‡æ ·
                probs = F.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)  # [B, 1]
            else:
                # Greedy decoding
                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # [B, 1]

            # === å¯¹å·²å®Œæˆçš„æ ·æœ¬ï¼Œä¸æ›´æ–° tokenï¼ˆç”¨åŸåºåˆ—æœ€åä¸€ä¸ª token å ä½ï¼‰===
            # æ³¨æ„ï¼šä¹Ÿå¯ä»¥ç”¨ pad_tokenï¼Œä½†æ¨¡å‹æœªå®šä¹‰ pad_token_idï¼Œæ•…å¤ç”¨ last token
            last_token = generated[:, -1:].clone()  # [B, 1]
            next_token = torch.where(finished.unsqueeze(1), last_token, next_token)

            # æ‹¼æ¥åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token], dim=1)  # [B, T+1]

            # === æ›´æ–° finished çŠ¶æ€ï¼ˆä»…å½“è®¾ç½®äº† stop_tokens æ—¶ï¼‰===
            if stop_tensor is not None:
                # æ£€æŸ¥æ–°ç”Ÿæˆçš„ token æ˜¯å¦åœ¨ stop_tokens ä¸­ â†’ [B]
                is_stop = (next_token == stop_tensor).any(dim=1)  # å¹¿æ’­æ¯”è¾ƒ [B,1] vs [S] â†’ [B,S] â†’ any â†’ [B]
                finished = finished | is_stop

    return generated
```

