# -*- coding: utf-8 -*-
"""
æ‰‹å†™ Qwen3-0.6B æ¨¡å‹æ¨ç†ä»£ç ï¼ˆæ—  KV Cacheï¼‰
é€‚ç”¨äºå­¦ä¹ ç›®çš„ï¼Œä½†åœ¨ä½æ˜¾å­˜è®¾å¤‡ï¼ˆå¦‚ 4GB GPUï¼‰ä¸Šéœ€è°¨æ…ä½¿ç”¨ã€‚
"""

import torch
from safetensors.torch import load_file  # å®‰å…¨åŠ è½½ .safetensors æƒé‡æ–‡ä»¶
from torch import nn
import torch.nn.functional as F
from tokenizers import Tokenizer  # ä½¿ç”¨ Hugging Face çš„ fast tokenizer


# ==============================================================================
# 1. RoPEï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰å®ç°
# Qwen3 ä½¿ç”¨ RoPE å¯¹ query å’Œ key è¿›è¡Œä½ç½®æ„ŸçŸ¥ç¼–ç 
# ==============================================================================
def apply_rotary_pos_emb(q, k, position_ids, head_dim, rope_theta=1000000.0):
    """
    å¯¹ query å’Œ key åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰

    Args:
        q: [batch_size, num_heads, seq_len, head_dim]
        k: [batch_size, num_key_value_heads, seq_len, head_dim]
        position_ids: [batch_size, seq_len]ï¼Œæ¯ä¸ª token çš„ä½ç½®ç´¢å¼•
        head_dim: æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼ˆQwen3 ä¸­ä¸º 128ï¼‰
        rope_theta: RoPE çš„åŸºåº•é¢‘ç‡å‚æ•°ï¼ˆQwen3 ä½¿ç”¨ 1e6ï¼‰

    Returns:
        q_embed, k_embed: ç»è¿‡ RoPE ç¼–ç åçš„ query å’Œ key
    """
    device = q.device
    # è®¡ç®—é¢‘ç‡åæ¯”ï¼šinv_freq = 1 / (theta^(i/d))ï¼Œi ä¸ºå¶æ•°ç´¢å¼•
    inv_freq = 1.0 / (
        rope_theta
        ** (torch.arange(0, head_dim, 2, dtype=torch.float32, device=device) / head_dim)
    )

    # freqs = position_ids * inv_freq â†’ [batch_size, seq_len, head_dim//2]
    freqs = position_ids.unsqueeze(-1).float() * inv_freq.unsqueeze(0).unsqueeze(0)

    # å°†é¢‘ç‡æ‰©å±•ä¸ºå®Œæ•´ç»´åº¦ï¼ˆå®éƒ¨+è™šéƒ¨ï¼‰
    emb = torch.cat([freqs, freqs], dim=-1)  # [batch_size, seq_len, head_dim]

    # è®¡ç®— cos å’Œ sinï¼Œå¹¶æ‰©å±•ç»´åº¦ä»¥åŒ¹é… q/k çš„å½¢çŠ¶ [batch, 1, seq_len, head_dim]
    cos = emb.cos().unsqueeze(1).to(q.dtype)
    sin = emb.sin().unsqueeze(1).to(q.dtype)

    def rotate_half(x):
        """å°†å‘é‡ååŠéƒ¨åˆ†ç§»åˆ°å‰é¢å¹¶å–è´Ÿï¼Œå®ç°å¤æ•°ä¹˜æ³•çš„æ—‹è½¬æ•ˆæœ"""
        x1, x2 = x.chunk(2, dim=-1)  # æ‹†åˆ†ä¸ºä¸¤åŠ
        return torch.cat((-x2, x1), dim=-1)

    # åº”ç”¨ RoPE: x * cos + rotate_half(x) * sin
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


# ==============================================================================
# 2. RMSNorm å®ç°ï¼ˆQwen3 ä½¿ç”¨çš„ LayerNorm å˜ä½“ï¼‰
# ä¸ä½¿ç”¨å¯å­¦ä¹ çš„ biasï¼Œä»…ç¼©æ”¾
# ==============================================================================
class SelfQwen3RMSNorm(torch.nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))  # å¯å­¦ä¹ ç¼©æ”¾å› å­
        self.variance_epsilon = eps  # é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°

    def forward(self, hidden_states):
        """
        RMSNorm å…¬å¼: x * weight / sqrt(mean(x^2) + eps)
        æ³¨æ„ï¼šå…ˆè½¬ä¸º float32 è®¡ç®—ä»¥é¿å…ç²¾åº¦æŸå¤±ï¼Œå†è½¬å›åŸ dtype
        """
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)  # åœ¨æœ€åä¸€ä¸ªç»´åº¦æ±‚å‡å€¼
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)


# ==============================================================================
# 3. å¤šå¤´æ³¨æ„åŠ›æ¨¡å—ï¼ˆæ”¯æŒ GQAï¼šGrouped Query Attentionï¼‰
# Qwen3-0.6B: num_heads=16, num_key_value_heads=8 â†’ æ¯ 2 ä¸ª Q å…±äº« 1 ä¸ª K/V
# ==============================================================================
class Attention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size  # 1024
        self.num_heads = config.num_attention_heads  # 16
        self.num_key_value_heads = config.num_key_value_heads  # 8
        self.head_dim = (
            config.head_dim
        )  # 128ï¼ˆæ³¨æ„ï¼šhidden_size â‰  num_heads * head_dimï¼ï¼‰
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads  # 2

        # æŠ•å½±å±‚ï¼šæ³¨æ„è¾“å‡ºç»´åº¦åŸºäº head_dim è€Œé hidden_size
        self.q_proj = nn.Linear(
            self.hidden_size, self.num_heads * self.head_dim, bias=False
        )
        self.k_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.v_proj = nn.Linear(
            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False
        )
        self.o_proj = nn.Linear(
            self.num_heads * self.head_dim, self.hidden_size, bias=False
        )

        # Qwen3 ç‰¹æœ‰ï¼šåœ¨ RoPE å‰å¯¹ Q/K åš RMSNorm
        self.q_norm = SelfQwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)
        self.k_norm = SelfQwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)
        self.rope_theta = config.rope_theta

    def forward(self, hidden_states, position_ids=None, attention_mask=None):
        bsz, q_len, _ = hidden_states.size()

        # æŠ•å½±å¹¶ reshape ä¸ºå¤šå¤´æ ¼å¼ [batch, num_heads, seq_len, head_dim]
        query_states = (
            self.q_proj(hidden_states)
            .view(bsz, q_len, self.num_heads, self.head_dim)
            .transpose(1, 2)
        )
        key_states = (
            self.k_proj(hidden_states)
            .view(bsz, q_len, self.num_key_value_heads, self.head_dim)
            .transpose(1, 2)
        )
        value_states = (
            self.v_proj(hidden_states)
            .view(bsz, q_len, self.num_key_value_heads, self.head_dim)
            .transpose(1, 2)
        )

        # Qwen3 ç‰¹æœ‰ï¼šå…ˆå¯¹ Q/K åš RMSNorm
        query_states = self.q_norm(query_states)
        key_states = self.k_norm(key_states)

        # å¦‚æœæœªæä¾› position_idsï¼Œåˆ™ä½¿ç”¨é»˜è®¤é€’å¢åºåˆ—
        if position_ids is None:
            position_ids = torch.arange(q_len, device=hidden_states.device).unsqueeze(0)

        # åº”ç”¨ RoPE ä½ç½®ç¼–ç 
        query_states, key_states = apply_rotary_pos_emb(
            query_states, key_states, position_ids, self.head_dim, self.rope_theta
        )

        # GQAï¼šå°† K/V æ‰©å±•ä»¥åŒ¹é… Q çš„å¤´æ•°ï¼ˆ8 heads â†’ 16 headsï¼‰
        if self.num_key_value_groups > 1:
            # æ‰©å±•ç»´åº¦å flatten åˆå¹¶
            key_states = (
                key_states.unsqueeze(2)
                .expand(-1, -1, self.num_key_value_groups, -1, -1)
                .flatten(1, 2)
            )
            value_states = (
                value_states.unsqueeze(2)
                .expand(-1, -1, self.num_key_value_groups, -1, -1)
                .flatten(1, 2)
            )

        # è®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        attn_weights = torch.matmul(query_states, key_states.transpose(-2, -1)) / (
            self.head_dim**0.5
        )

        # åº”ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆå¦‚å› æœæ©ç ï¼‰
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask

        # softmax å½’ä¸€åŒ–ï¼ˆæ³¨æ„ï¼šå…ˆè½¬ float32 æé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
        attn_weights = torch.softmax(attn_weights, dim=-1, dtype=torch.float32).to(
            hidden_states.dtype
        )
        attn_output = torch.matmul(attn_weights, value_states)

        # åˆå¹¶æ³¨æ„åŠ›å¤´å¹¶æŠ•å½±å› hidden_size
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, -1)
        attn_output = self.o_proj(attn_output)
        return attn_output


# ==============================================================================
# 4. MLP æ¨¡å—ï¼ˆSwiGLU æ¿€æ´»ï¼‰
# Qwen3 ä½¿ç”¨ gate_proj + up_proj + silu æ¿€æ´»
# ==============================================================================
class MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.gate_proj = nn.Linear(
            config.hidden_size, config.intermediate_size, bias=False
        )
        self.up_proj = nn.Linear(
            config.hidden_size, config.intermediate_size, bias=False
        )
        self.down_proj = nn.Linear(
            config.intermediate_size, config.hidden_size, bias=False
        )

    def forward(self, x):
        # SwiGLU: down_proj(silu(gate_proj(x)) * up_proj(x))
        return self.down_proj(F.silu(self.gate_proj(x)) * self.up_proj(x))


# ==============================================================================
# 5. å•ä¸ª Decoder å±‚ï¼ˆAttention + MLP + æ®‹å·®è¿æ¥ï¼‰
# ==============================================================================
class DecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.self_attn = Attention(config)
        self.input_layernorm = SelfQwen3RMSNorm(config.hidden_size)  # Pre-norm ç»“æ„
        self.post_attention_layernorm = SelfQwen3RMSNorm(config.hidden_size)
        self.mlp = MLP(config)

    def forward(self, hidden_states, position_ids=None, attention_mask=None):
        # ç¬¬ä¸€ä¸ªæ®‹å·®å—ï¼šAttention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.self_attn(hidden_states, position_ids, attention_mask)
        hidden_states = residual + hidden_states

        # ç¬¬äºŒä¸ªæ®‹å·®å—ï¼šMLP
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states
        return hidden_states


# ==============================================================================
# 6. å®Œæ•´è¯­è¨€æ¨¡å‹ï¼ˆQwen3ForCausalLMï¼‰
# ==============================================================================
class Qwen3ForCausalLM(nn.Module):
    def __init__(self, config):
        print("Initializing Qwen3 model...")
        super().__init__()
        self.config = config
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList(
            [DecoderLayer(config) for _ in range(config.num_hidden_layers)]
        )
        self.norm = SelfQwen3RMSNorm(config.hidden_size)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # è¯åµŒå…¥ä¸è¾“å‡ºå±‚æƒé‡å…±äº«ï¼ˆtie embeddingsï¼‰
        self.lm_head.weight = self.embed_tokens.weight

    def forward(self, input_ids, attention_mask=None, position_ids=None):
        bsz, q_len = input_ids.shape

        # è‡ªåŠ¨ç”Ÿæˆä½ç½® IDï¼ˆä» 0 åˆ° seq_len-1ï¼‰
        position_ids = torch.arange(
            q_len, dtype=torch.long, device=input_ids.device
        ).unsqueeze(0)

        # æ„å»ºå› æœæ³¨æ„åŠ›æ©ç ï¼ˆä¸‹ä¸‰è§’ä¸º 0ï¼Œä¸Šä¸‰è§’ä¸º -infï¼‰
        causal_mask = torch.triu(
            torch.full(
                (q_len, q_len),
                float("-inf"),
                dtype=torch.float32,
                device=input_ids.device,
            ),
            diagonal=1,
        )
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # [1, 1, L, L]

        # åµŒå…¥è¾“å…¥ tokens
        hidden_states = self.embed_tokens(input_ids)

        # é€å±‚å‰å‘ä¼ æ’­
        for layer in self.layers:
            hidden_states = layer(
                hidden_states, position_ids=position_ids, attention_mask=causal_mask
            )

        # æœ€ç»ˆå½’ä¸€åŒ– + è¯­è¨€æ¨¡å‹å¤´
        hidden_states = self.norm(hidden_states)
        logits = self.lm_head(hidden_states)
        return logits


# ==============================================================================
# 7. Qwen3-0.6B æ¨¡å‹é…ç½®ï¼ˆç¡¬ç¼–ç ï¼‰
# æ¥è‡ªå®˜æ–¹ config.json
# ==============================================================================
class SelfQwen3Config:
    architectures = ["Qwen3ForCausalLM"]
    attention_bias = False
    attention_dropout = 0.0
    bos_token_id = 151643  # Begin-of-Sequence token
    eos_token_id = 151645  # End-of-Sequence token
    head_dim = 128  # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦
    hidden_act = "silu"
    hidden_size = 1024  # éšè—å±‚ç»´åº¦
    initializer_range = 0.02
    intermediate_size = 3072  # MLP ä¸­é—´å±‚å¤§å°
    max_position_embeddings = 40960
    num_attention_heads = 16
    num_hidden_layers = 28  # æ€»å…± 28 å±‚
    num_key_value_heads = 8  # GQA è®¾ç½®
    rms_norm_eps = 1e-06
    rope_theta = 1000000  # RoPE åŸºé¢‘
    tie_word_embeddings = True
    vocab_size = 151936  # è¯è¡¨å¤§å°ï¼ˆåŒ…å«ç‰¹æ®Š tokenï¼‰


# ==============================================================================
# 8. ä¸»å‡½æ•°ï¼šåŠ è½½æ¨¡å‹ + ç”Ÿæˆæ–‡æœ¬
# âš ï¸ æ³¨æ„ï¼šæ­¤å®ç°æ—  KV Cacheï¼Œæ•ˆç‡ä½ï¼Œæ˜¾å­˜å ç”¨é«˜ï¼
# ==============================================================================
def main():
    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆCPU ä¸Šï¼‰
    config = SelfQwen3Config()
    model = Qwen3ForCausalLM(config)

    # ä» safetensors æ–‡ä»¶åŠ è½½æƒé‡
    state_dict = load_file("model/model.safetensors")

    # ç§»é™¤æƒé‡å­—å…¸ä¸­çš„ "model." å‰ç¼€ï¼ˆé€‚é… Hugging Face æ ¼å¼ï¼‰
    new_state_dict = {}
    for k, v in state_dict.items():
        if k.startswith("model."):
            new_state_dict[k[len("model.") :]] = v  # å»æ‰ "model."
        else:
            new_state_dict[k] = v

    # åŠ è½½æƒé‡ï¼ˆä¸¥æ ¼åŒ¹é…ï¼‰
    model.load_state_dict(new_state_dict, strict=True)
    model.eval()
    print("âœ… Model loaded successfully!")

    # åŠ è½½åˆ†è¯å™¨
    tokenizer = Tokenizer.from_file("model/tokenizer.json")

    # æ„é€ èŠå¤©æ¨¡æ¿ï¼ˆâš ï¸ æ­¤å¤„æœªä½¿ç”¨å®˜æ–¹ apply_chat_templateï¼Œå¯èƒ½æ ¼å¼ä¸æ ‡å‡†ï¼‰
    message = "<|im_start|>useræ˜å¤©åšç‚¹å•¥<|im_end|>><|im_start|>assistant"
    input_ids = tokenizer.encode(message).ids
    input_ids = torch.tensor([input_ids], dtype=torch.long)

    print(f"Input token IDs shape: {input_ids.shape}")

    # ğŸ”¥ è‡ªå›å½’ç”Ÿæˆï¼ˆæ—  KV Cacheï¼æ¯æ¬¡é‡æ–°è®¡ç®—å…¨éƒ¨å†å²ï¼‰
    with torch.no_grad():
        for step in range(1000):  # æœ€å¤šç”Ÿæˆ 1000 ä¸ª tokenï¼ˆææ˜“ OOMï¼ï¼‰
            # å‰å‘æ¨ç†ï¼ˆâš ï¸ æ•´ä¸ªåºåˆ—é‡æ–°è®¡ç®—ï¼ï¼‰
            logits = model(input_ids)

            # å–æœ€åä¸€ä¸ª token çš„ logits å¹¶ greedy é‡‡æ ·
            next_token_logits = logits[:, -1, :] / 1.0
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            # é‡åˆ° EOS token åˆ™åœæ­¢
            if next_token.item() == config.eos_token_id:
                break

            # æ‹¼æ¥æ–° token
            input_ids = torch.cat([input_ids, next_token], dim=1)

        # è§£ç è¾“å‡ºï¼ˆè·³è¿‡ç‰¹æ®Š tokenï¼‰
        output_text = tokenizer.decode(input_ids[0].tolist(), skip_special_tokens=True)
        print("\nğŸ¤– Generated Output:")
        print(output_text)


if __name__ == "__main__":
    main()
