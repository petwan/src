# minigpt/dataset_stats.py

import json
import argparse
from tokenizer import Tokenizer
import matplotlib.pyplot as plt
from typing import List, Dict
import numpy as np


def get_num_tokens(file_path: str, tokenizer: Tokenizer) -> List[int]:
    """
    è¯»å– JSONL æ–‡ä»¶ï¼Œå¯¹æ¯æ¡æ ·æœ¬çš„ question + answer è¿›è¡Œ tokenizeï¼Œ
    è¿”å›æ¯ä¸ªæ ·æœ¬çš„ token æ•°é‡åˆ—è¡¨ã€‚
    """
    input_num_tokens = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    question = data.get("question", "")
                    answer = data.get("answer", "")
                    tokens, _ = tokenizer.encode(
                        question, answer, pad_to_max_length=False
                    )
                    input_num_tokens.append(len(tokens))
                except json.JSONDecodeError:
                    print(
                        f"âš ï¸ ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ã€‚å†…å®¹: {line[:50]}..."
                    )
                except KeyError as e:
                    print(f"âš ï¸ ç¬¬ {line_num} è¡Œç¼ºå°‘å­—æ®µ {e}ï¼Œè·³è¿‡ã€‚")
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨ï¼")
        raise SystemExit(1)
    return input_num_tokens


def count_intervals(
    num_tokens: List[int], interval: int = 20, max_length: int = None
) -> Dict[str, int]:
    """æŒ‰åŒºé—´ç»Ÿè®¡ token é•¿åº¦åˆ†å¸ƒ"""
    if not num_tokens:
        return {}

    actual_max = max(num_tokens)
    upper_bound = min(actual_max, max_length) if max_length else actual_max

    intervals_count = {}
    current = 0
    while current <= upper_bound:
        next_bound = current + interval
        count = sum(1 for x in num_tokens if current <= x < next_bound)
        intervals_count[f"{current}-{next_bound}"] = count
        current = next_bound

    if max_length and actual_max > max_length:
        overflow_count = sum(1 for x in num_tokens if x >= max_length)
        intervals_count[f">{max_length}"] = overflow_count

    return intervals_count


def plot_token_distribution(
    num_tokens: List[int],
    intervals_count: Dict[str, int],
    interval: int = 20,
    max_length: int = 512,
    title: str = "Token åˆ†å¸ƒ",
):
    """ç»˜åˆ¶å¹¶æ˜¾ç¤º token é•¿åº¦åˆ†å¸ƒæŸ±çŠ¶å›¾ï¼Œå¹¶æ ‡å‡º 90% åˆ†ä½ç«–çº¿"""
    if not intervals_count or not num_tokens:
        print("ğŸ“Š æ— æ•°æ®å¯ç»˜å›¾ã€‚")
        return

    # è®¡ç®— 95% åˆ†ä½æ•°
    p95 = np.percentile(num_tokens, 95)

    # æ„å»ºæ•°å€¼ x åæ ‡å’Œå¯¹åº”çš„æ ‡ç­¾
    x_vals = []
    labels = []
    y_vals = []

    current = 0
    # æ·»åŠ å¸¸è§„åŒºé—´
    while True:
        key = f"{current}-{current + interval}"
        if key in intervals_count:
            x_vals.append(current + interval / 2)  # åŒºé—´ä¸­ç‚¹
            labels.append(key)
            y_vals.append(intervals_count[key])
            current += interval
        else:
            break

    # æ·»åŠ æº¢å‡ºåŒºé—´ï¼ˆå¦‚æœæœ‰ï¼‰
    overflow_key = f">{max_length}"
    if overflow_key in intervals_count:
        # å°†æº¢å‡ºåŒºé—´æ”¾åœ¨ max_length å³ä¾§ä¸€ç‚¹ï¼Œé¿å…é‡å 
        x_vals.append(max_length + interval / 2)
        labels.append(overflow_key)
        y_vals.append(intervals_count[overflow_key])

    fig, ax = plt.subplots(figsize=(12, 6))

    # ç»˜åˆ¶æŸ±çŠ¶å›¾
    bars = ax.bar(
        x_vals, y_vals, width=interval * 0.8, color="lightcoral", edgecolor="black"
    )

    # è®¾ç½® x è½´
    ax.set_xticks(x_vals)
    ax.set_xticklabels(labels, rotation=45, ha="right")

    # ç”» 90% åˆ†ä½ç«–çº¿
    ax.axvline(
        p95,
        color="blue",
        linestyle="--",
        linewidth=2,
        label=f"95% Percentile ({p95:.1f})",
    )

    # æ·»åŠ æŸ±å­é¡¶éƒ¨çš„æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        if height > 0:
            ax.text(
                bar.get_x() + bar.get_width() / 2,
                height,
                f"{int(height)}",
                ha="center",
                va="bottom",
                fontsize=9,
            )

    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    ax.set_title(title, fontsize=14)
    ax.set_ylabel("Sample Nbr", fontsize=12)
    ax.set_xlabel("Token Length", fontsize=12)
    ax.legend()

    plt.tight_layout()
    plt.show()


def main():
    parser = argparse.ArgumentParser(description="åˆ†æè®­ç»ƒé›† Token é•¿åº¦åˆ†å¸ƒ")
    parser.add_argument(
        "--train_data",
        type=str,
        required=True,
        help="è®­ç»ƒæ•°æ®è·¯å¾„ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ª {'question': ..., 'answer': ...}ï¼‰",
    )
    parser.add_argument(
        "--vocab_path",
        type=str,
        required=True,
        help="è¯è¡¨æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºåˆå§‹åŒ– Tokenizerï¼‰",
    )
    parser.add_argument(
        "--interval", type=int, default=20, help="ç»Ÿè®¡åŒºé—´æ­¥é•¿ï¼ˆé»˜è®¤: 20ï¼‰"
    )
    parser.add_argument(
        "--max_length",
        type=int,
        default=512,
        help="æœ€å¤§ç»Ÿè®¡é•¿åº¦ï¼Œè¶…è¿‡çš„å½’å…¥ '>max_length' åŒºé—´ï¼ˆé»˜è®¤: 512ï¼‰",
    )
    parser.add_argument(
        "--no_plot", action="store_true", help="ä¸æ˜¾ç¤ºå›¾è¡¨ï¼ˆä»…æ‰“å°ç»Ÿè®¡ç»“æœï¼‰"
    )

    args = parser.parse_args()

    # åˆå§‹åŒ–åˆ†è¯å™¨
    try:
        tokenizer = Tokenizer(args.vocab_path)
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        raise SystemExit(1)

    # è·å– token é•¿åº¦
    print(f"ğŸ” æ­£åœ¨å¤„ç†æ•°æ®: {args.train_data}")
    num_tokens = get_num_tokens(args.train_data, tokenizer)
    print(f"âœ… å…±åŠ è½½ {len(num_tokens)} æ¡æœ‰æ•ˆæ ·æœ¬ã€‚")

    if not num_tokens:
        print("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œé€€å‡ºã€‚")
        return

    # ç»Ÿè®¡åˆ†å¸ƒ
    intervals = count_intervals(
        num_tokens, interval=args.interval, max_length=args.max_length
    )

    # æ‰“å°ç»“æœ
    print("\nğŸ“Š Token é•¿åº¦åˆ†å¸ƒç»Ÿè®¡:")
    total = 0
    for interval_label, count in intervals.items():
        print(f"  {interval_label:>12}: {count:>6}")
        total += count
    print(f"{'-' * 20}\n  æ€»è®¡: {total}")

    # å¯é€‰ï¼šç»˜å›¾ï¼ˆç°åœ¨ä¼ å…¥åŸå§‹ num_tokensï¼‰
    if not args.no_plot:
        plot_token_distribution(
            num_tokens=num_tokens,
            intervals_count=intervals,
            interval=args.interval,
            max_length=args.max_length,
            title=f"Token Length Distribution ({args.train_data})",
        )


if __name__ == "__main__":
    main()
