# minigpt

> æ‰§è¡Œä»£ç 
```bash
# 1. åˆ›å»ºè¯è¡¨
python ./minigpt/build_vocab.py --data ./data/data.jsonl --output ./data/vocab.json
# 2. æ‹†åˆ†æ•°æ®é›†
python ./minigpt/split_data.py --input ./data/data.jsonl --train_ratio 0.8
# 3. è®­ç»ƒæ¨¡å‹
python ./minigpt/train.py
```
## 1. æ„å»ºè¯è¡¨

### 1.1 `build_vocab.py` â€” è¯è¡¨æ„å»ºå·¥å…·

è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œå…ˆæŒ‰ç…§ä¸­æ–‡å­—ç¬¦ç”Ÿæˆè¯è¡¨ï¼Œå…·ä½“å¦‚ä¸‹ï¼š
- **åˆ†è¯å•ä½**ï¼šæ¯ä¸€ä¸ª**å•å­—**ï¼ˆåŒ…æ‹¬æ±‰å­—ã€æ ‡ç‚¹ã€æ•°å­—ã€è‹±æ–‡å­—æ¯ç­‰ï¼‰ä½œä¸ºä¸€ä¸ª tokenã€‚
- **ç¤ºä¾‹**ï¼š
  - æ–‡æœ¬ï¼š`"ä½ å¥½å—ï¼Ÿ"`
  - åˆ†è¯ç»“æœï¼š`["ä½ ", "å¥½", "å—", "ï¼Ÿ"]`
  - æ¯ä¸ªå­—å¯¹åº”ä¸€ä¸ª ID

- **ç‰¹æ®Š Token**ï¼š
  - `<pad>`ï¼šå¡«å……
  - `<unk>`ï¼šæœªç™»å½•å­—ï¼ˆç†è®ºä¸Šä¸ä¼šå‡ºç°ï¼Œå› ä¸ºä½ ç”¨å…¨è®­ç»ƒé›†æ„å»ºè¯è¡¨ï¼‰
  - `<sep>`ï¼šåˆ†éš”ç¬¦ï¼ˆç”¨äºåˆ†éš” question å’Œ answerï¼‰


ç”¨äºä»é—®ç­”ï¼ˆQAï¼‰æ•°æ®é›†ä¸­æå–æ‰€æœ‰å­—ç¬¦ï¼Œå¹¶ç”Ÿæˆæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„è¯è¡¨æ–‡ä»¶ `vocab.json`ã€‚è¯¥è¯è¡¨å°†è¢« `Tokenizer` ç±»åŠ è½½ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„ token ID åºåˆ—ã€‚

**è¾“å…¥æ•°æ®æ ¼å¼**
è„šæœ¬è¦æ±‚è¾“å…¥æ–‡ä»¶ä¸º **JSONLï¼ˆJSON Linesï¼‰æ ¼å¼**ï¼Œå³æ¯è¡ŒåŒ…å«ä¸€ä¸ªç‹¬ç«‹çš„ JSON å¯¹è±¡ï¼Œä¸”å¿…é¡»åŒ…å« `question` å’Œ `answer` å­—æ®µã€‚

**ç¤ºä¾‹ (`data/data.json`)**ï¼š
```json
{"question": "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ", "answer": "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"}
{"question": "ä½ å–œæ¬¢æ—…è¡Œå—ï¼Ÿ", "answer": "æ˜¯çš„ï¼Œæˆ‘éå¸¸å–œæ¬¢ã€‚"}
```
> âš ï¸ æ³¨æ„ï¼šä¸æ˜¯æ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ª JSON æ•°ç»„ï¼Œè€Œæ˜¯**æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡**ã€‚

```bash
python build_vocab.py --data <è®­ç»ƒæ•°æ®è·¯å¾„> [--output <è¾“å‡ºè·¯å¾„>]
```

```bash
python ./minigpt/build_vocab.py --data ./data/data.jsonl --output ./data/vocab.json
```

- **ç‰¹æ®Š token å›ºå®šåŒ…å«**ï¼š`<pad>`ï¼ˆå¡«å……ï¼‰ã€`<unk>`ï¼ˆæœªçŸ¥å­—ç¬¦ï¼‰ã€`<sep>`ï¼ˆåˆ†éš”ç¬¦ï¼‰
- æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ã€æ ‡ç‚¹ã€æ•°å­—ã€å­—æ¯ç­‰å‡æŒ‰ Unicode æ’åºååˆ†é… ID
- è¯è¡¨å¤§å° = 3ï¼ˆç‰¹æ®Š tokenï¼‰ + å”¯ä¸€å­—ç¬¦æ•°

> **æ³¨æ„**ï¼šå¦‚æœæ›´æ–°äº†æ•°æ®é›†ï¼Œéœ€è¦é‡æ–°è¿è¡Œæ­¤è„šæœ¬ä»¥æ›´æ–°è¯è¡¨ã€‚

```python
import enum
import json
import argparse
from collections import Counter


def build_vocab(data_path: str, output_path: str):
    counter = Counter()

    with open(data_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            try:
                item = json.loads(line)
                # æ”¶é›† question å’Œ answer ä¸­çš„æ‰€æœ‰å­—ç¬¦
                counter.update(item["question"])
                counter.update(item["answer"])
            except Exception as e:
                print(f"Warning: skip invalid line: {line[:50]}... | Error: {e}")

    # è·å–æ‰€æœ‰å”¯ä¸€å­—ç¬¦
    chars = sorted(counter.keys())

    # æ·»åŠ ç‰¹æ®Š token
    special_tokens = ["<pad>", "<unk>", "<sep>"]

    # æ„å»º word2idï¼šå…ˆæ”¾ç‰¹æ®Š tokenï¼Œå†æ”¾å­—ç¬¦ï¼ˆé¡ºåºå›ºå®šä¾¿äºå¤ç°ï¼‰
    word2id = {token: i for i, token in enumerate(special_tokens)}
    for char in chars:
        if char not in special_tokens:  # é˜²å¾¡ï¼šè·³è¿‡ç‰¹æ®Š token
            word2id[char] = len(word2id)  # è‡ªåŠ¨é€’å¢

    # æ„å»º id2word
    id2word = {i: token for token, i in word2id.items()}

    vocab = {"word2id": word2id, "id2word": id2word}

    # ä¿å­˜ vocab
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(vocab, f, ensure_ascii=False, indent=4)

    print(f"âœ… Vocabulary built and saved to {output_path}")
    print(f"   Total tokens: {len(word2id)}")
    print(f"   Special tokens: {special_tokens}")
    print(f"   Sample chars: {chars[:10]}...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Build vocabulary from QA dataset.")
    parser.add_argument(
        "--data", type=str, required=True, help="Path to training data (JSONL format)"
    )
    parser.add_argument(
        "--output", type=str, default="vocab.json", help="Output vocab file path"
    )
    args = parser.parse_args()

    build_vocab(args.data, args.output)
```

## 2. tokenizer

åœ¨æ„å»ºè¯è¡¨åï¼Œåˆ›å»ºå¯¹åº”çš„ Tokenizer ç±»ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„ token ID åºåˆ—ã€‚

```python
import json
import token


class Tokenizer:
    def __init__(self, vocab_path: str):
        with open(vocab_path, "r", encoding="utf-8") as f:
            vocab = json.load(f)

        self.word2id = vocab["word2id"]
        self.id2word = {int(k): v for k, v in vocab["id2word"].items()}

        # å›ºå®šç‰¹æ®Š token ID
        self.pad_token_id = self.word2id["<pad>"]
        self.unk_token_id = self.word2id["<unk>"]
        self.sep_token_id = self.word2id["<sep>"]

    def encode(
        self,
        question: str,
        answer: str,
        max_length: int = 128,
        pad_to_max_length: bool = True,
    ):
        """å°†é—®ç­”å¯¹ç¼–ç ä¸º token ID åºåˆ—ã€‚"""
        tokens = []

        # encode question
        for char in question:
            tokens.append(self.word2id.get(char, self.unk_token_id))
        tokens.append(self.sep_token_id)  # æ·»åŠ åˆ†éš”ç¬¦

        # encode answer
        if answer is not None:
            for char in answer:
                tokens.append(self.word2id.get(char, self.unk_token_id))

            tokens.append(self.sep_token_id)

        # æ„å»º attention maskï¼ˆ1=çœŸå® tokenï¼Œ0=paddingï¼‰
        attn_mask = [1] * len(tokens)

        # æˆªæ–­æˆ–å¡«å……
        if pad_to_max_length:
            if len(tokens) > max_length:
                # æˆªæ–­ï¼ˆä¿ç•™å¼€å¤´ï¼‰
                tokens = tokens[:max_length]
                attn_mask = attn_mask[:max_length]
            else:
                # å¡«å……
                pad_len = max_length - len(tokens)
                tokens.extend([self.pad_token_id] * pad_len)
                attn_mask.extend([0] * pad_len)

        return tokens, attn_mask

    def decode(self, ids):
        """å°† token ID åˆ—è¡¨è§£ç ä¸ºåŸå§‹æ–‡æœ¬ï¼ˆè·³è¿‡ <pad>ï¼‰ã€‚"""
        return "".join(
            self.id2word[i] for i in ids if i != self.pad_token_id  # è·³è¿‡å¡«å……ç¬¦
        )

    def get_vocab_size(self):
        return len(self.id2word)

```

æµ‹è¯•ä¸€ä¸‹ï¼š
```python
if __name__ == "__main__":
    question = "ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ"
    answer = "æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼"

    tokenizer = Tokenizer("./data/vocab.json")

    input_ids, attn_mask = tokenizer.encode(question, answer, max_length=32)
    print(input_ids)
    print(tokenizer.decode(input_ids))
    """------ result ------
    [368, 1086, 4810, 2005, 4169, 1521, 240, 2103, 4816, 2, 1646, 1480, 1086, 4810, 3989, 3989, 4806, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ä½ å¥½ï¼Œæœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ<sep>æˆ‘å¾ˆå¥½ï¼Œè°¢è°¢ï¼<sep>
    """"

```

## 3. æ‹†åˆ†æ•°æ®é›†
æˆ‘ä»¬å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œå¹¶ä¿å­˜ä¸º JSONL æ–‡ä»¶ã€‚

```python
# split_data.py
import json
import argparse
import random
from pathlib import Path


def split_jsonl_data(input_path: str, train_ratio: float = 0.9, seed: int = 42):
    """
    å°† JSONL æ ¼å¼çš„ QA æ•°æ®é›†åˆ’åˆ†ä¸º train.jsonl å’Œ val.jsonlã€‚

    Args:
        input_path (str): åŸå§‹æ•°æ®è·¯å¾„ï¼ˆJSONLï¼‰
        train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0.0 ï½ 1.0ï¼‰
        seed (int): éšæœºç§å­ï¼Œç¡®ä¿å¯å¤ç°
    """
    input_path = Path(input_path)
    if not input_path.exists():
        raise FileNotFoundError(f"Input file not found: {input_path}")

    # è¯»å–æ‰€æœ‰æœ‰æ•ˆè¡Œ
    data = []
    with open(input_path, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                if "question" in item and "answer" in item:
                    data.append(line)  # ä¿ç•™åŸå§‹å­—ç¬¦ä¸²ï¼Œé¿å…æ ¼å¼å˜åŒ–
                else:
                    print(
                        f"âš ï¸  Warning: Line {line_num} missing 'question' or 'answer', skipped."
                    )
            except json.JSONDecodeError:
                print(f"âš ï¸  Warning: Line {line_num} is invalid JSON, skipped.")

    if not data:
        raise ValueError("No valid data found!")

    # æ‰“ä¹±å¹¶åˆ’åˆ†
    random.seed(seed)
    random.shuffle(data)
    n_train = int(len(data) * train_ratio)

    train_data = data[:n_train]
    val_data = data[n_train:]

    # è¾“å‡ºè·¯å¾„
    output_dir = input_path.parent
    train_path = output_dir / "train.jsonl"
    val_path = output_dir / "val.jsonl"

    # å†™å…¥æ–‡ä»¶
    with open(train_path, "w", encoding="utf-8") as f:
        f.write("\n".join(train_data) + "\n")
    with open(val_path, "w", encoding="utf-8") as f:
        f.write("\n".join(val_data) + "\n")

    print(f"âœ… Split completed!")
    print(f"   Total samples: {len(data)}")
    print(f"   Train: {len(train_data)} â†’ {train_path}")
    print(f"   Val:   {len(val_data)} â†’ {val_path}")
    print(f"   Train ratio: {train_ratio:.1%}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Split QA dataset into train/val sets."
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="Path to original JSONL dataset (e.g., data/all.jsonl)",
    )
    parser.add_argument(
        "--train_ratio",
        type=float,
        default=0.9,
        help="Proportion of data to use for training (default: 0.9)",
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="Random seed for reproducibility (default: 42)",
    )
    args = parser.parse_args()

    split_jsonl_data(args.input, args.train_ratio, args.seed)
```

```bash
python ./minigpt/split_data.py --input ./data/data.jsonl --train_ratio 0.8
```

## 4. æ„å»ºDataset
æ„å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„ PyTorch Dataset ç±»ï¼Œç”¨äºåŠ è½½æ•°æ®é›†å¹¶ç”Ÿæˆè¾“å…¥åºåˆ—å’Œæ ‡ç­¾ã€‚

```python
import torch
from torch.utils.data import Dataset, DataLoader
import json
from minigpt.tokenizer import Tokenizer


class QADataset(Dataset):
    """
    è‡ªå›å½’è®­ç»ƒç”¨çš„é—®ç­”æ•°æ®é›†ï¼ˆQuestion-Answer Datasetï¼‰ã€‚

    å°† (question, answer) æ‹¼æ¥ä¸ºå•ä¸ªåºåˆ—ï¼Œå¹¶æ„é€ ï¼š
      - input_ids:   [SOS, q1, q2, ..., <sep>, a1, a2, ..., <sep>]
      - targets:     [q1, q2, ..., <sep>, a1, a2, ..., <sep>, EOS]

    å®é™…é€šè¿‡ shift å®ç°ï¼šinput = tokens[:-1], target = tokens[1:]
    """

    def __init__(self, data_path: str, tokenizer: Tokenizer, max_length: int):
        self.tokeniner = tokenizer
        self.max_length = max_length
        self.data = []

        print(f"Loading data from {data_path}")

        with open(data_path, "r", encoding="utf-8") as f:
            # é€è¡Œè¯»å–æ•°æ®, è¡Œæ•°ä»1å¼€å§‹
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line:
                    continue

                try:
                    item = json.loads(line)
                    if "question" not in item or "answer" not in item:
                        print(
                            f"âš ï¸  Line {line_num}: Missing 'question' or 'answer', skipped."
                        )
                        continue
                    self.data.append((item["question"], item["answer"]))

                except Exception as e:
                    print(f"âš ï¸  Line {line_num}: Invalid JSON, skipped. Error: {e}")

        print(f"âœ… Loaded {len(self.data)} valid QA pairs.")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        question, answer = self.data[idx]
        # encode question and answer
        full_tokens, atnn_mask = self.tokeniner.encode(
            question, answer, max_length=self.max_length
        )

        # è‡ªå›å½’è®­ç»ƒï¼š input å‘å³ç§»ä¸€ä½ï¼Œtarget å‘å·¦ç§»ä¸€ä½
        input_ids = full_tokens[:-1]  # å»æ‰æœ€åä¸€ä¸ª token
        attention_mask = atnn_mask[:-1]  # å¯¹åº” input_ids çš„ attention mask
        targets = full_tokens[1:]  # å»æ‰ç¬¬ä¸€ä¸ª token

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "targets": torch.tensor(targets, dtype=torch.long),
        }


if __name__ == "__main__":
    tokenizer = Tokenizer("./data/vocab.json")
    dataset = QADataset("./data/train.jsonl", tokenizer, max_length=128)

    print(f"æ•°æ®é›†å¤§å°ï¼š{len(dataset)}")

    item = dataset[0]
    print(item)
    print(tokenizer.decode(item["input_ids"].tolist()))
    print(tokenizer.decode(item["targets"].tolist()))
```

æµ‹è¯•ä¸€ä¸‹
```python
if __name__ == "__main__":
    tokenizer = Tokenizer("./data/vocab.json")
    dataset = QADataset("./data/train.jsonl", tokenizer, max_length=128)

    print(f"æ•°æ®é›†å¤§å°ï¼š{len(dataset)}")

    item = dataset[0]
    print(item)
    print(tokenizer.decode(item["input_ids"].tolist()))
    print(tokenizer.decode(item["targets"].tolist()))
"""
æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©åˆ†ç±»ä¸ºä½•ç±»ï¼Ÿ<sep>æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©å±äºæœå®ç±»ã€‚<sep>
æ ‘ç”Ÿé•¿çš„äº§ç‰©åˆ†ç±»ä¸ºä½•ç±»ï¼Ÿ<sep>æ£æ ‘ç”Ÿé•¿çš„äº§ç‰©å±äºæœå®ç±»ã€‚<sep>
"""
```

## 5. å…ˆæŠŠè®­ç»ƒé€»è¾‘å†™å¥½

TODO: æ·»åŠ ä»£ç 

## 6. GPTLMHeadModel

è¿™é‡Œé‡‡ç”¨çš„æ˜¯ decoder-only çš„ GPT æ¨¡å‹ï¼Œå³åªåŒ…å«è§£ç å™¨éƒ¨åˆ†ï¼Œä¸åŒ…å«ç¼–ç å™¨éƒ¨åˆ†ã€‚

æ•´ä½“æ€è·¯ï¼Œç»è¿‡Transformeræ¨¡å‹åçš„ hidden_state ä½œä¸ºè¾“å…¥ï¼Œç»™åˆ°ä¸€ä¸ªnn.Linear å±‚ï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

```python
from dataclasses import dataclass
import torch.nn as nn
import torch
from typing import Optional
import torch.nn.functional as F


@dataclass
class GPTConfig:
    """
    æ¨¡å‹è¶…å‚æ•°é…ç½®ç±»ï¼ˆä½¿ç”¨ dataclass ç®€æ´å®šä¹‰ï¼‰

    é»˜è®¤å€¼å‚è€ƒ GPT-2 smallï¼Œä½† vocab_size æ ¹æ®ä¸­æ–‡ä»»åŠ¡è°ƒæ•´
    """

    n_embd: int = 768  # token embedding ç»´åº¦ï¼ˆä¹Ÿå³ hidden sizeï¼‰
    n_head: int = 8  # å¤šå¤´æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¿…é¡»æ•´é™¤ n_embdï¼‰
    n_layer: int = 6  # Transformer block å±‚æ•°
    dropout: float = 0.1  # æ‰€æœ‰ dropout å±‚çš„ä¸¢å¼ƒç‡
    block_size: int = 128  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆä½ç½®ç¼–ç æœ€å¤§æ”¯æŒé•¿åº¦ï¼‰
    vocab_size: int = 4825  # è¯è¡¨å¤§å°ï¼ˆæ ¹æ®å®é™… tokenizer å†³å®š


class CausalSelfAttention(nn.Module):
    def __init__(self, config: GPTConfig):
        super().__init__()

        assert config.n_embd % config.n_head == 0
        self.config = config
        # å°† embedding åˆ†æˆäº†å¤šä¸ª head
        self.head_dim = config.n_embd // config.n_head
        # åˆ›å»º queryã€keyã€value æŠ•å½±
        # å°† [B, T, C] ä¸€æ¬¡æ€§æŠ•å½±ä¸º [B, T, 3 * C]ï¼Œç„¶åå†æ‹†åˆ†ä¸º Q/K/V
        self.qkv_proj = nn.Linear(config.n_embd, 3 * config.n_embd)

        # è¾“å‡ºæŠ•å½±ï¼šå°†å¤šå¤´æ‹¼æ¥åçš„å‘é‡æ˜ å°„å›åŸå§‹ç»´åº¦
        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)

        # æ³¨æ„åŠ›è¾“å‡ºçš„æ®‹å·®è¿æ¥ååŠ  dropout
        self.resid_dropput = nn.Dropout(config.dropout)

    def forward(
        self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, n_embd]
            attention_mask: å¯é€‰ï¼Œpadding æ©ç ï¼Œå½¢çŠ¶ [batch_size, seq_len]
                - 1 è¡¨ç¤ºæœ‰æ•ˆ token
                - 0 è¡¨ç¤º padding token

        Returns:
            è¾“å‡ºå¼ é‡ï¼Œå½¢çŠ¶ [batch_size, seq_len, n_embd]
        """
        # batch, sequence length, embedding dim
        B, T, C = x.shape

        # === æ­¥éª¤1: QKV èåˆæŠ•å½± ===
        qkv = self.qkv_proj(x)
        # æ‹†åˆ†æˆ Q, K, V, ç»´åº¦å‡æ˜¯ [B, T, C]
        q, k, v = qkv.chunk(3, dim=-1)

        # === æ­¥éª¤2: å¤šå¤´åˆ†å‰²ä¸è½¬ç½® ===
        # å°†æ¯ä¸ªå¤´çš„ç»´åº¦ä»Cæ‹†ä¸º n_head, head_dim
        # ç„¶åè½¬ç½®ä¸º [B, n_head, T, head_dim] ä»¥ä¾¿è¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜
        q = q.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)
        k = k.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)
        v = v.view(B, T, self.config.n_head, self.head_dim).transpose(1, 2)

        # === æ­¥éª¤3: æ„å»ºå¤åˆæ³¨æ„åŠ›æ©ç  ===
        attn_mask = None
        if attention_mask is not None:
            # padding æ©ç : [B, T] -> bool
            key_padding_mask = attention_mask.to(torch.bool)

            # å› æœæ©ç ï¼šä¸‹ä¸‰è§’çŸ©é˜µ[T, T]ï¼Œç¡®ä¿åªèƒ½ attend åˆ°å½“å‰åŠå·¦ä¾§
            causal_mask = torch.tril(
                torch.ones(T, T, dtype=torch.bool, device=x.device)
            )

            # æ‹“å±•ç»´åº¦ä»¥æ”¯æŒå¹¿æ’­
            # key_padding_mask: [B, 1, 1, T]
            # causal_mask: [1, 1, T, T]
            key_padding_mask = key_padding_mask[:, None, None, :]
            causal_mask = causal_mask[None, None, :, :]

            # é€»è¾‘ä¸ï¼šåªæœ‰åŒæ—¶æ»¡è¶³â€œé paddingâ€å’Œâ€œåœ¨å·¦ä¾§â€æ‰ä¸º True
            attn_mask = causal_mask & key_padding_mask  # [B, 1, T, T]

            # === æ­¥éª¤4: é«˜æ•ˆæ³¨æ„åŠ›è®¡ç®— ===
            y = F.scaled_dot_product_attention(
                q,
                k,
                v,
                attn_mask=attn_mask,
                dropout_p=self.config.dropout if self.training else 0,
                is_causal=False,  # NOTEï¼šæˆ‘ä»¬å·²æ‰‹åŠ¨æ„å»ºå› æœæ©ç ï¼Œæ•…è®¾ä¸º False
            )

            # è¾“å‡º y: [B, n_head, T, head_dim]

        # === æ­¥éª¤5: åˆå¹¶å¤šå¤´å¹¶æŠ•å½± ===
        # è½¬ç½®å› [B, T, n_head, head_dim] -> åˆå¹¶æœ€åä¸¤ç»´ -> [B, T, C]
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        # ç»è¿‡è¾“å‡ºæŠ•å½± + æ®‹å·® dropout
        y = self.resid_dropout(self.out_proj(y))
        return y


class MLP(nn.Module):
    """
    å‰é¦ˆç¥ç»ç½‘ç»œï¼ˆFeed-Forward Network, FFNï¼‰

    GPT æ ‡å‡†ç»“æ„ï¼š
      Linear(n_embd, 4*n_embd) â†’ GELU â†’ Linear(4*n_embd, n_embd)

    ä¸ºä»€ä¹ˆæ˜¯ 4 å€ï¼Ÿâ€”â€” GPT è®ºæ–‡å‘ç°æ­¤æ¯”ä¾‹åœ¨æ€§èƒ½ä¸è®¡ç®—é—´å–å¾—è‰¯å¥½å¹³è¡¡
    """

    def __init__(self, config: GPTConfig):
        super().__init__()

        # å‡ç»´ï¼šæ‰©å¤§è¡¨ç¤ºèƒ½åŠ›
        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)
        # é™ç»´ï¼šå›åˆ°åŸå§‹ç»´åº¦
        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)
        # è¾“å‡º dropout
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.c_fc(x)
        x = F.gelu(x)  # GELU æ¿€æ´»å‡½æ•°ï¼ˆGPT ç³»åˆ—æ ‡å‡†é€‰æ‹©ï¼‰
        x = self.c_proj(x)
        x = self.dropout(x)
        return x


class Block(nn.Module):
    """
    Transformer è§£ç å™¨å—ï¼ˆDecoder Blockï¼‰

    ç»“æ„ï¼ˆPre-LN é£æ ¼ï¼‰ï¼š
      x â†’ LayerNorm â†’ Attention â†’ Add â†’ LayerNorm â†’ MLP â†’ Add â†’ Output

    Pre-LN ä¼˜åŠ¿ï¼š
      - è®­ç»ƒæ›´ç¨³å®šï¼ˆæ¢¯åº¦ä¸ä¼šéšå±‚æ•°çˆ†ç‚¸ï¼‰
      - æ— éœ€å­¦ä¹ ç‡ warmupï¼ˆåœ¨ä¸­å°æ¨¡å‹ä¸­æ•ˆæœæ˜¾è‘—ï¼‰
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        # ç¬¬ä¸€ä¸ª LayerNormï¼ˆç”¨äº Attention å‰ï¼‰
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        # ç¬¬äºŒä¸ª LayerNormï¼ˆç”¨äº MLP å‰ï¼‰
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = MLP(config)

    def forward(
        self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        # æ³¨æ„åŠ›å­å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        x = x + self.attn(self.ln_1(x), attention_mask)
        # MLP å­å±‚ï¼ˆå¸¦æ®‹å·®è¿æ¥ï¼‰
        x = x + self.mlp(self.ln_2(x))
        return x


class Transformer(nn.Module):
    """
    åŒ…å«ï¼š
      - Token Embedding
      - å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼ˆLearned Positional Embeddingï¼‰
      - N ä¸ª Block
      - æœ€ç»ˆ LayerNormï¼ˆPre-LN ç»“æ„çš„ä¸€éƒ¨åˆ†ï¼‰
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config

        # Token Embedding: [vocab_size, n_embd]
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)

        # ä½ç½®ç¼–ç : [block_size, n_embd]ï¼ˆå¯å­¦ä¹ ï¼Œéæ­£å¼¦ï¼‰
        # æ³¨æ„ï¼šGPT-2 ä½¿ç”¨å¯å­¦ä¹ ä½ç½®ç¼–ç ï¼Œè€ŒéåŸå§‹ Transformer çš„å›ºå®šç¼–ç 
        self.wpe = nn.Embedding(config.block_size, config.n_embd)

        # è¾“å…¥ dropout
        self.drop = nn.Dropout(config.dropout)

        # å †å  N ä¸ª Transformer Block
        self.blocks = nn.ModuleList([Block(config) for _ in range(config.n_layer)])

        # æœ€ç»ˆ LayerNormï¼ˆPre-LN ç»“æ„è¦æ±‚åœ¨æœ€ååŠ ä¸€æ¬¡ LNï¼‰
        self.ln_f = nn.LayerNorm(config.n_embd)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            input_ids: [B, T]ï¼Œtoken ID åºåˆ—
            attention_mask: [B, T]ï¼Œå¯é€‰ padding æ©ç 

        Returns:
            hidden_states: [B, T, n_embd]
        """
        B, T = input_ids.shape
        # å®‰å…¨æ£€æŸ¥ï¼šåºåˆ—é•¿åº¦ä¸èƒ½è¶…è¿‡ block_size
        assert (
            T <= self.config.block_size
        ), f"åºåˆ—é•¿åº¦ {T} è¶…å‡ºæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ {self.config.block_size}"

        # === è·å– token embedding ===
        tok_emb = self.wte(input_ids)  # [B, T, n_embd]

        # === è·å–ä½ç½® embedding ===
        # ä½¿ç”¨ arange ç”Ÿæˆä½ç½®ç´¢å¼• [0, 1, 2, ..., T-1]
        pos = torch.arange(0, T, dtype=torch.long, device=input_ids.device)
        pos_emb = self.wpe(pos)  # [T, n_embd]

        # === åˆå¹¶ token + ä½ç½® embedding ===
        x = self.drop(tok_emb + pos_emb)  # [B, T, n_embd]

        # === é€šè¿‡æ‰€æœ‰ Transformer Block ===
        for block in self.blocks:
            x = block(x, attention_mask)

        # === æœ€ç»ˆ LayerNorm ===
        x = self.ln_f(x)
        return x


class GPTLMHeadModel(nn.Module):
    """
    å®Œæ•´çš„ GPT è¯­è¨€æ¨¡å‹ï¼ˆå«è¯­è¨€å»ºæ¨¡å¤´ï¼‰

    å…³é”®ç‰¹æ€§ï¼š
      - æƒé‡ç»‘å®šï¼ˆWeight Tyingï¼‰ï¼šwte.weight = lm_head.weight
        * å‡å°‘å‚æ•°é‡
        * æå‡è®­ç»ƒç¨³å®šæ€§ï¼ˆPress & Wolf, 2017ï¼‰
      - è‡ªå›å½’è¯­è¨€å»ºæ¨¡ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
    """

    def __init__(self, config: GPTConfig):
        super().__init__()
        self.config = config
        self.transformer = Transformer(config)

        # è¯­è¨€å»ºæ¨¡å¤´ï¼šå°† hidden state æ˜ å°„åˆ° vocab logits
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)

        # ğŸ”‘ æƒé‡ç»‘å®šï¼šå…±äº« embedding ä¸ lm_head çš„æƒé‡çŸ©é˜µ
        # æ³¨æ„ï¼šå¿…é¡»åœ¨åˆå§‹åŒ–åç«‹å³ç»‘å®šï¼Œä¸”ä¸¤è€… bias å‡ä¸º False
        self.transformer.wte.weight = self.lm_head.weight

        # åˆå§‹åŒ–æ‰€æœ‰å‚æ•°
        self.apply(self._init_weights)

    def _init_weights(self, module):
        """
        å‚æ•°åˆå§‹åŒ–ç­–ç•¥ï¼ˆéµå¾ª GPT-2ï¼‰
        - Linear / Embedding: Normal(0, 0.02)
        - Bias: å…¨é›¶ï¼ˆä½†æœ¬æ¨¡å‹æ—  biasï¼‰
        """
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(
        self, input_ids: torch.Tensor, attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å…¥ token IDsï¼Œè¾“å‡ºæ¯ä¸ªä½ç½®çš„è¯æ±‡è¡¨ logits

        Args:
            input_ids: [B, T]ï¼Œè¾“å…¥ token ID åºåˆ—
            attention_mask: [B, T]ï¼Œå¯é€‰ï¼Œç”¨äºå±è”½ padding

        Returns:
            logits: [B, T, vocab_size]ï¼Œæ¯ä¸ªä½ç½®å¯¹æ•´ä¸ªè¯è¡¨çš„é¢„æµ‹åˆ†æ•°
        """
        # é€šè¿‡ Transformer ç¼–ç å™¨è·å–éšè—çŠ¶æ€
        hidden_states = self.transformer(input_ids, attention_mask)

        # é€šè¿‡è¯­è¨€å»ºæ¨¡å¤´å¾—åˆ° logits
        logits = self.lm_head(hidden_states)  # [B, T, vocab_size]

        return logits
```

## 7. æ¨¡å‹è®­ç»ƒ
```python
import torch
from minigpt.qa_dataset import QADataset
from minigpt.tokenizer import Tokenizer
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import torch.nn as nn
import os
from minigpt.model import GPTLMHeadModel, GPTConfig
from tqdm import tqdm


def train_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    num_batches = len(train_loader)

    progress_bar = tqdm(
        enumerate(train_loader),
        total=num_batches,
        desc="  Train",
        leave=False,
        unit="batch",
    )

    for batch_idx, batch in progress_bar:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)

        optimizer.zero_grad()
        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / num_batches


@torch.no_grad()
def validate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    num_batches = len(val_loader)

    progress_bar = tqdm(
        enumerate(val_loader),
        total=num_batches,
        desc="  Val",
        leave=False,
        unit="batch",
    )

    for batch_idx, batch in progress_bar:
        input_ids = batch["input_ids"].to(device, non_blocking=True)
        attention_mask = batch["attention_mask"].to(device, non_blocking=True)
        targets = batch["targets"].to(device, non_blocking=True)

        logits = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))
        total_loss += loss.item()
        progress_bar.set_postfix({"loss": f"{loss.item():.4f}"})

    return total_loss / num_batches


def save_checkpoint(model, optimizer, epoch, path):
    torch.save(
        {
            "epoch": epoch,
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
        },
        path,
    )


def train_model(
    model,
    train_loader,
    val_loader,
    optimizer,
    criterion,
    device,
    num_epochs,
    model_output_dir,
    writer,
):
    os.makedirs(model_output_dir, exist_ok=True)
    best_val_loss = float("inf")

    for epoch in range(1, num_epochs + 1):
        print(f"\nEpoch {epoch}/{num_epochs}")

        # Training
        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)
        print(f"  Train Loss: {train_loss:.4f}")

        # Validation
        val_loss = validate(model, val_loader, criterion, device)
        print(f"  Val Loss:   {val_loss:.4f}")

        # Log to TensorBoard
        if writer is not None:
            writer.add_scalar("Loss/train", train_loss, epoch)
            writer.add_scalar("Loss/val", val_loss, epoch)

        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_path = os.path.join(model_output_dir, "best_model.pth")
            save_checkpoint(model, optimizer, epoch, best_path)
            print(f"  ğŸ‰ New best model saved (val loss: {val_loss:.4f})")

    print(f"\nâœ… Training finished. Best validation loss: {best_val_loss:.4f}")


def main():
    # é…ç½®è·¯å¾„
    train_path = "./data/train.jsonl"
    val_path = "./data/val.jsonl"
    vocab_path = "./data/vocab.json"

    # è¶…å‚æ•°
    max_length = 128
    batch_size = 32
    lr = 1e-4
    epochs = 15

    # è®¾å¤‡è®¾ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    if device.type == "cuda":
        print(f"GPU: {torch.cuda.get_device_name(0)}")

    # åŠ è½½ tokenizer å’Œæ¨¡å‹
    tokenizer = Tokenizer(vocab_path)
    config = GPTConfig(vocab_size=tokenizer.get_vocab_size())
    model = GPTLMHeadModel(config).to(device)

    # æ•°æ®é›†
    train_dataset = QADataset(train_path, tokenizer, max_length)
    val_dataset = QADataset(val_path, tokenizer, max_length)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2,
        pin_memory=True,
        drop_last=True,
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2,
        pin_memory=True,
        drop_last=True,
    )

    # ä¼˜åŒ–å™¨ä¸æŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # TensorBoard æ—¥å¿—
    writer = SummaryWriter("runs/minigpt")

    # å¼€å§‹è®­ç»ƒ
    train_model(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        criterion=criterion,
        device=device,
        num_epochs=epochs,
        model_output_dir="output",
        writer=writer,
    )

    writer.close()
    print("\nğŸ‰ Training pipeline completed.")


if __name__ == "__main__":
    main()

```

```bash
python ./minigpt/train.py
```

## 8. ç”Ÿæˆå›ç­”


## X. Qwen3
Qwen3çš„ä»£ç ç»“æ„å­¦ä¹ ï¼Œè¯¦è§ minigpt/qwen3 ä»£ç ã€‚
